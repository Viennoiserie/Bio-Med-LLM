{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installer les bibliotèques nécessaires :\n",
    "\n",
    "Pour PyTDC, nous n'installons pas les dépendances car tileDBsoma (l'une de ces dernières) a de gros problème de compatibilité (en tout cas sur ma machine)\n",
    "\n",
    "Les dépendances requises pour PyTDC sont celles-ci : numpy, pandas, scipy et torch et seront installées indépendamment\n",
    "\n",
    "Lien documentation TDC : https://tdc.readthedocs.io/en/main/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.48.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: setuptools in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (75.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: biopython in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.84)\n",
      "Requirement already satisfied: numpy in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from biopython) (2.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: anndata in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: array-api-compat!=1.5,>1.4 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anndata) (1.10.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\thoma\\appdata\\roaming\\python\\python310\\site-packages (from anndata) (1.2.2)\n",
      "Requirement already satisfied: h5py>=3.7 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anndata) (3.12.1)\n",
      "Requirement already satisfied: natsort in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anndata) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anndata) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anndata) (24.2)\n",
      "Requirement already satisfied: pandas!=2.1.0rc0,!=2.1.2,>=1.4 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anndata) (2.2.3)\n",
      "Requirement already satisfied: scipy>1.8 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anndata) (1.15.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: mygene in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.2.2)\n",
      "Requirement already satisfied: biothings-client>=0.2.6 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mygene) (0.4.0)\n",
      "Requirement already satisfied: httpx>=0.22.0 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from biothings-client>=0.2.6->mygene) (0.28.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx>=0.22.0->biothings-client>=0.2.6->mygene) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\thoma\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.0+cu124)\n",
      "Requirement already satisfied: filelock in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scipy in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.15.1)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scipy) (2.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: PyTDC in c:\\users\\thoma\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install setuptools\n",
    "%pip install biopython\n",
    "%pip install anndata\n",
    "%pip install pandas\n",
    "%pip install mygene\n",
    "%pip install torch\n",
    "%pip install scipy\n",
    "%pip install numpy\n",
    "\n",
    "%pip install PyTDC --no-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Charger le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le dataset\n",
    "aml = pd.read_csv('../data processing/reduced_dataset/aml_anova.csv', sep=\",\", on_bad_lines=\"skip\")\n",
    "\n",
    "# Transposer les données et mettre en forme le DataFrame\n",
    "data = aml.transpose()\n",
    "\n",
    "data.columns = data.iloc[0]\n",
    "data = data[1:]\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Préparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. Charger scGPT et son Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from tdc import tdc_hf_interface\n",
    "from tdc.model_server.tokenizers.scgpt import scGPTTokenizer\n",
    "\n",
    "# Charger le modèle scGPT via TDC HuggingFace interface\n",
    "scgpt = tdc_hf_interface(\"scGPT\")\n",
    "model = scgpt.load() \n",
    "\n",
    "# Charger le tokenizer spécifique pour scGPT\n",
    "tokenizer = scGPTTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Préparer les données aux bons formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type des valeurs d'expression : float32\n",
      "Type des noms des gènes : <class 'pandas.core.indexes.base.Index'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Récupérer les noms des gènes à partir du dataset\n",
    "gene_names = data.columns[:-2] \n",
    "\n",
    "# Exclure les labels et ne conserver que les valeurs d'expression\n",
    "data_values = data.iloc[:, :-2].values  \n",
    "\n",
    "# Convertir les valeurs en float (en cas de type non numérique)\n",
    "data_values = data_values.astype(np.float32)\n",
    "\n",
    "# Remplacer les NaN (si présents) \n",
    "data_values = np.nan_to_num(data_values, nan=0.0)\n",
    "\n",
    "# Vérifier les types des données avant de les tokeniser\n",
    "print(f\"Type des valeurs d'expression : {data_values.dtype}\")\n",
    "print(f\"Type des noms des gènes : {type(gene_names)}\")\n",
    "\n",
    "# Convertir les noms des gènes en liste de chaînes si ce n'est pas déjà le cas\n",
    "if not isinstance(gene_names, list):\n",
    "    gene_names = gene_names.tolist()\n",
    "\n",
    "# Ajouter le jeton '<cls>' au début de la liste des gènes\n",
    "gene_names = ['<cls>'] + gene_names\n",
    "\n",
    "# Ajouter une colonne vide (ou des zéros) à data_values pour faire correspondre le nombre de gènes\n",
    "data_values = np.hstack([np.zeros((data_values.shape[0], 1), dtype=np.float32), data_values])\n",
    "\n",
    "# Tokeniser les données en utilisant la méthode tokenize_cell_vectors\n",
    "tokenized_data = tokenizer.tokenize_cell_vectors(data_values, np.array(gene_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Préparation du chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "chunk_sizes = [3, 5, 10, 20, 400]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\\n\")\n",
    "model.to(device)\n",
    "\n",
    "def process_in_chunks(tokenized_data, chunk_size):\n",
    "\n",
    "    dataloader = DataLoader(tokenized_data, batch_size=chunk_size, shuffle=False)\n",
    "    row_embeddings = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "\n",
    "        input_ids, gene_tokens = batch\n",
    "        \n",
    "        # Créer un masque d'attention (mask) pour les gènes (pas de padding ici)\n",
    "        mask = torch.tensor(gene_tokens != 0, dtype=torch.bool).to(device)\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        gene_tokens = gene_tokens.to(device)\n",
    "\n",
    "        # Inférence par lots\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs = model(input_ids, gene_tokens, attention_mask=mask)\n",
    "            row_embeddings.append(outputs['cell_emb'].cpu().numpy())\n",
    "\n",
    "    # Convertir les embeddings en un tableau NumPy\n",
    "    return np.vstack(row_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Embedding et exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing with chunk size: 3\n",
      "Batch input_ids shape: torch.Size([3, 158]), gene_tokens shape: torch.Size([3, 158])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;66;03m# Inférence par lots\u001b[39;00m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 21\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgene_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgene_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m             row_embeddings\u001b[38;5;241m.\u001b[39mappend(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcell_emb\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Padder des embeddings\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tdc\\model_server\\models\\scgpt.py:269\u001b[0m, in \u001b[0;36mScGPTModel.forward\u001b[1;34m(self, input_ids, values, attention_mask, output_cell_emb)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    input_ids: Tensor of gene indices, shape [batch_size, seq_len]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;124;03m        - 'zero_probs': Zero probabilities (if config.explicit_zero_prob=True)\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# Gene embeddings\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m gene_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgene_encoder\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m gene_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgene_encoder[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menc_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m](gene_emb)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# Value encoding\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "\n",
    "    print(f\"Processing with chunk size: {chunk_size}\")\n",
    "\n",
    "    # Utiliser le DataLoader sans padding\n",
    "    dataloader = DataLoader(tokenized_data, batch_size=chunk_size, shuffle=False)\n",
    "\n",
    "    row_embeddings = []\n",
    "\n",
    "    # Traiter chaque batch\n",
    "    for batch in dataloader:\n",
    "\n",
    "        input_ids, gene_tokens = batch\n",
    "        print(f\"Batch input_ids shape: {input_ids.shape}, gene_tokens shape: {gene_tokens.shape}\")\n",
    "\n",
    "        # Inférence par lots\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, gene_tokens, attention_mask=(gene_tokens != 0).to(device))\n",
    "            row_embeddings.append(outputs['cell_emb'])\n",
    "\n",
    "# Padder des embeddings\n",
    "padded_row_embeddings = pad_sequence(row_embeddings, batch_first=True, padding_value=0)\n",
    "\n",
    "# Convertir les embeddings en un tableau NumPy\n",
    "row_embeddings = padded_row_embeddings.cpu().numpy()\n",
    "print(f\"Embeddings shape après padding: {row_embeddings.shape}\")\n",
    "\n",
    "# Exporter les embeddings\n",
    "output_dir = f\"embeddings_chunk_{chunk_size}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, f\"scgpt_aml_embeddings_chunk_{chunk_size}.csv\")\n",
    "\n",
    "# Ajouter les labels aux embeddings\n",
    "pd_embeddings = pd.DataFrame(row_embeddings)\n",
    "\n",
    "embeddings_set = pd.concat([data.iloc[:, -2:], pd_embeddings], axis=1)\n",
    "embeddings_set.to_csv(output_path, index=False, header=True)\n",
    "\n",
    "# Vérifier les résultats\n",
    "csv = pd.read_csv(output_path, sep=\",\", on_bad_lines=\"skip\")\n",
    "\n",
    "print(f\"Shape des embeddings (chunk {chunk_size}):\", row_embeddings.shape)\n",
    "print(csv.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
