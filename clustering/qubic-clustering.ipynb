{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocks: ['BC000\\tS=460\\tEnrichment:1.94\\tRow=20\\tCol=23\\tCore_Row=20\\tCore_Col=23', 'BC001\\tS=480\\tEnrichment:1.93\\tRow=15\\tCol=32\\tCore_Row=15\\tCore_Col=32', 'BC002\\tS=435\\tEnrichment:1.91\\tRow=15\\tCol=29\\tCore_Row=15\\tCore_Col=29', 'BC003\\tS=270\\tEnrichment:1.97\\tRow=15\\tCol=18\\tCore_Row=15\\tCore_Col=18', 'BC004\\tS=434\\tEnrichment:1.92\\tRow=14\\tCol=31\\tCore_Row=14\\tCore_Col=31', 'BC005\\tS=546\\tEnrichment:1.94\\tRow=13\\tCol=42\\tCore_Row=13\\tCore_Col=42', 'BC006\\tS=442\\tEnrichment:1.91\\tRow=13\\tCol=34\\tCore_Row=13\\tCore_Col=34', 'BC007\\tS=221\\tEnrichment:2.01\\tRow=13\\tCol=17\\tCore_Row=13\\tCore_Col=17', 'BC008\\tS=288\\tEnrichment:1.74\\tRow=24\\tCol=12\\tCore_Row=24\\tCore_Col=12', 'BC009\\tS=540\\tEnrichment:1.91\\tRow=12\\tCol=45\\tCore_Row=12\\tCore_Col=45', 'BC010\\tS=288\\tEnrichment:1.92\\tRow=12\\tCol=24\\tCore_Row=12\\tCore_Col=24', 'BC011\\tS=240\\tEnrichment:1.98\\tRow=12\\tCol=20\\tCore_Row=12\\tCore_Col=20', 'BC012\\tS=516\\tEnrichment:1.90\\tRow=12\\tCol=43\\tCore_Row=12\\tCore_Col=43', 'BC013\\tS=252\\tEnrichment:1.97\\tRow=12\\tCol=21\\tCore_Row=12\\tCore_Col=21', 'BC014\\tS=528\\tEnrichment:1.95\\tRow=11\\tCol=48\\tCore_Row=11\\tCore_Col=48', 'BC015\\tS=495\\tEnrichment:1.92\\tRow=11\\tCol=45\\tCore_Row=11\\tCore_Col=45', 'BC016\\tS=308\\tEnrichment:1.98\\tRow=11\\tCol=28\\tCore_Row=11\\tCore_Col=28', 'BC017\\tS=350\\tEnrichment:1.97\\tRow=10\\tCol=35\\tCore_Row=10\\tCore_Col=35', 'BC018\\tS=210\\tEnrichment:2.00\\tRow=10\\tCol=21\\tCore_Row=10\\tCore_Col=21', 'BC019\\tS=486\\tEnrichment:1.96\\tRow=9\\tCol=54\\tCore_Row=9\\tCore_Col=54', 'BC020\\tS=171\\tEnrichment:2.01\\tRow=19\\tCol=9\\tCore_Row=19\\tCore_Col=9', 'BC021\\tS=176\\tEnrichment:1.95\\tRow=8\\tCol=22\\tCore_Row=8\\tCore_Col=22', 'BC022\\tS=336\\tEnrichment:1.92\\tRow=8\\tCol=42\\tCore_Row=8\\tCore_Col=42', 'BC023\\tS=88\\tEnrichment:1.99\\tRow=8\\tCol=11\\tCore_Row=8\\tCore_Col=11', 'BC024\\tS=224\\tEnrichment:1.90\\tRow=8\\tCol=28\\tCore_Row=8\\tCore_Col=28', 'BC025\\tS=175\\tEnrichment:2.02\\tRow=25\\tCol=7\\tCore_Row=20\\tCore_Col=7', 'BC026\\tS=133\\tEnrichment:1.96\\tRow=19\\tCol=7\\tCore_Row=19\\tCore_Col=7', 'BC027\\tS=98\\tEnrichment:2.12\\tRow=14\\tCol=7\\tCore_Row=12\\tCore_Col=7', 'BC028\\tS=175\\tEnrichment:1.95\\tRow=7\\tCol=25\\tCore_Row=7\\tCore_Col=25', 'BC029\\tS=91\\tEnrichment:1.92\\tRow=7\\tCol=13\\tCore_Row=7\\tCore_Col=13', 'BC030\\tS=270\\tEnrichment:1.93\\tRow=6\\tCol=45\\tCore_Row=6\\tCore_Col=45', 'BC031\\tS=252\\tEnrichment:1.95\\tRow=6\\tCol=42\\tCore_Row=6\\tCore_Col=42', 'BC032\\tS=132\\tEnrichment:2.09\\tRow=22\\tCol=6\\tCore_Row=22\\tCore_Col=6', 'BC033\\tS=174\\tEnrichment:1.97\\tRow=6\\tCol=29\\tCore_Row=6\\tCore_Col=29', 'BC034\\tS=144\\tEnrichment:1.94\\tRow=6\\tCol=24\\tCore_Row=6\\tCore_Col=24', 'BC035\\tS=230\\tEnrichment:1.95\\tRow=5\\tCol=46\\tCore_Row=5\\tCore_Col=46', 'BC036\\tS=260\\tEnrichment:1.95\\tRow=5\\tCol=52\\tCore_Row=5\\tCore_Col=52', 'BC037\\tS=215\\tEnrichment:1.99\\tRow=5\\tCol=43\\tCore_Row=5\\tCore_Col=43', 'BC038\\tS=100\\tEnrichment:1.97\\tRow=20\\tCol=5\\tCore_Row=20\\tCore_Col=5', 'BC039\\tS=152\\tEnrichment:1.83\\tRow=38\\tCol=4\\tCore_Row=38\\tCore_Col=4', 'BC040\\tS=92\\tEnrichment:1.92\\tRow=23\\tCol=4\\tCore_Row=21\\tCore_Col=4', 'BC041\\tS=108\\tEnrichment:2.04\\tRow=27\\tCol=4\\tCore_Row=25\\tCore_Col=4', 'BC042\\tS=87\\tEnrichment:2.09\\tRow=29\\tCol=3\\tCore_Row=29\\tCore_Col=3', 'BC043\\tS=105\\tEnrichment:1.92\\tRow=35\\tCol=3\\tCore_Row=34\\tCore_Col=3', 'BC044\\tS=117\\tEnrichment:2.02\\tRow=39\\tCol=3\\tCore_Row=27\\tCore_Col=3', 'BC045\\tS=279\\tEnrichment:1.99\\tRow=3\\tCol=93\\tCore_Row=3\\tCore_Col=93', 'BC046\\tS=201\\tEnrichment:1.88\\tRow=3\\tCol=67\\tCore_Row=3\\tCore_Col=67', 'BC047\\tS=144\\tEnrichment:1.94\\tRow=3\\tCol=48\\tCore_Row=3\\tCore_Col=48', 'BC048\\tS=138\\tEnrichment:1.91\\tRow=3\\tCol=46\\tCore_Row=3\\tCore_Col=46']\n",
      "Genes: [['24', '69', '54', '11', '74', '90', '79', '129', '49', '84', '32', '104', '36', '116', '25', '119', '130', '99', '110', '51'], ['13', '129', '90', '69', '54', '49', '11', '74', '79', '84', '24', '32', '104', '116', '36'], ['10', '19', '138', '72', '109', '37', '95', '128', '153', '78', '2', '113', '117', '103', '27'], ['120', '145', '57', '49', '69', '54', '90', '129', '84', '119', '36', '24', '51', '104', '116'], ['27', '50', '19', '37', '72', '138', '109', '78', '95', '113', '2', '103', '117', '153'], ['19', '37', '72', '138', '109', '78', '95', '113', '128', '153', '2', '117', '103'], ['18', '129', '69', '90', '49', '54', '11', '79', '84', '74', '119', '99', '116'], ['67', '112', '132', '37', '19', '109', '78', '138', '72', '95', '23', '62', '113'], ['39', '123', '28', '47', '92', '102', '107', '105', '26', '68', '111', '58', '0', '7', '55', '93', '44', '41', '22', '133', '45', '137', '155', '82'], ['69', '129', '54', '49', '90', '11', '79', '74', '84', '104', '116', '99'], ['7', '65', '21', '88', '55', '121', '60', '91', '64', '146', '44', '93'], ['112', '132', '78', '19', '109', '37', '138', '72', '95', '27', '103', '117'], ['48', '69', '11', '90', '129', '54', '74', '79', '84', '49', '104', '116'], ['113', '157', '95', '138', '19', '109', '37', '72', '128', '2', '117', '27'], ['37', '112', '19', '78', '109', '72', '138', '95', '128', '153', '113'], ['23', '37', '19', '109', '72', '78', '138', '95', '113', '62', '128'], ['95', '126', '113', '138', '19', '109', '37', '72', '78', '23', '153'], ['14', '95', '19', '109', '78', '37', '138', '72', '23', '62'], ['19', '75', '95', '138', '72', '78', '37', '109', '23', '62'], ['20', '112', '37', '78', '109', '19', '72', '138', '95'], ['47', '142', '28', '123', '92', '105', '39', '68', '26', '102', '58', '0', '107', '108', '111', '88', '7', '21', '93'], ['62', '94', '152', '3', '2', '113', '73', '128'], ['69', '120', '129', '49', '119', '54', '90', '84'], ['127', '154', '131', '143', '34', '3', '73', '53'], ['69', '118', '79', '11', '49', '90', '54', '84'], ['19', '156', '78', '72', '109', '37', '138', '95', '2', '128', '153', '103', '62', '112', '127', '73', '122', '46', '3', '94', '64', '84', '88', '91', '121'], ['131', '149', '143', '59', '12', '73', '97', '35', '101', '63', '16', '87', '80', '53', '127', '100', '158', '17', '18'], ['69', '115', '49', '129', '54', '79', '8', '11', '90', '18', '57', '84', '27', '95'], ['96', '127', '131', '2', '46', '153', '103'], ['46', '147', '128', '117', '50', '2', '62'], ['1', '32', '69', '11', '74', '24'], ['131', '150', '127', '153', '128', '2'], ['4', '45', '82', '7', '133', '155', '71', '21', '39', '94', '111', '58', '14', '62', '152', '3', '27', '50', '63', '65', '68', '107'], ['6', '63', '128', '153', '2', '50'], ['134', '136', '18', '8', '29', '66'], ['15', '32', '141', '104', '144'], ['18', '29', '134', '8', '49'], ['30', '113', '95', '138', '19'], ['3', '33', '133', '27', '28', '92', '47', '0', '58', '123', '68', '26', '102', '39', '105', '82', '107', '111', '45', '7'], ['7', '21', '55', '93', '44', '102', '123', '28', '107', '92', '26', '47', '105', '39', '68', '58', '111', '121', '64', '124', '41', '81', '60', '146', '22', '71', '45', '137', '82', '83', '91', '88', '114', '140', '27', '98', '66', '155'], ['56', '61', '43', '97', '66', '86', '125', '34', '100', '127', '131', '143', '31', '52', '12', '57', '59', '70', '77', '80', '101', '158', '82'], ['42', '127', '131', '76', '53', '12', '73', '97', '59', '101', '87', '80', '143', '100', '148', '17', '18', '89', '31', '5', '9', '52', '77', '85', '153', '19', '72'], ['39', '108', '68', '58', '47', '28', '123', '0', '92', '105', '26', '102', '107', '111', '93', '7', '44', '45', '133', '137', '94', '82', '155', '55', '41', '22', '27', '140', '3'], ['76', '131', '127', '34', '63', '153', '2', '62', '103', '46', '122', '59', '101', '73', '35', '53', '12', '43', '97', '148', '8', '17', '18', '29', '49', '69', '70', '79', '80', '87', '100', '134', '151', '158', '37'], ['16', '63', '153', '128', '2', '46', '122', '103', '10', '56', '19', '37', '95', '109', '113', '138', '72', '20', '112', '62', '78', '117', '125', '94', '27', '38', '143', '7', '11', '21', '54', '64', '69', '79', '84', '88', '90', '102', '129'], ['40', '74', '11'], ['49', '106', '129'], ['5', '139', '53'], ['135', '143', '131']]\n",
      "Conditions: [['cg00715696', 'cg01515515', 'cg02137956', 'cg05206633', 'cg05445326', 'cg07793808', 'cg08173263', 'cg09156207', 'cg10741025', 'cg10940462', 'cg11417025', 'cg14473743', 'cg17413460', 'cg18411150', 'cg19047660', 'cg20793665', 'cg22222281', 'cg23530553', 'cg23531049', 'cg25496181', 'cg25801976', 'cg26376241', 'hsa.mir.1248'], ['CT45A3.441519', 'cg00715696', 'cg00997251', 'cg01515515', 'cg01538731', 'cg02137956', 'cg03982355', 'cg05206633', 'cg05445326', 'cg06474225', 'cg07793808', 'cg08173263', 'cg08347500', 'cg09426383', 'cg09936008', 'cg10741025', 'cg11417025', 'cg12252069', 'cg14473743', 'cg17413460', 'cg18411150', 'cg19047660', 'cg20300129', 'cg20513976', 'cg21387009', 'cg22222281', 'cg23531049', 'cg25496181', 'cg25801976', 'cg26147845', 'cg26376241', 'hsa.mir.1248'], ['cg00532474', 'cg01252023', 'cg02239258', 'cg03956042', 'cg03972560', 'cg04857395', 'cg07195011', 'cg08698943', 'cg09232555', 'cg09462281', 'cg09975576', 'cg10473367', 'cg12057563', 'cg12448285', 'cg13074055', 'cg14381350', 'cg14499274', 'cg14511923', 'cg14531862', 'cg15288800', 'cg15417249', 'cg18384588', 'cg19537719', 'cg22731271', 'cg23171972', 'cg23528247', 'cg23690893', 'cg26235215', 'cg26607828'], ['CT45A3.441519', 'GPR142.350383', 'cg01515515', 'cg02137956', 'cg10741025', 'cg10940462', 'cg13481969', 'cg13979887', 'cg14473743', 'cg17413460', 'cg20513976', 'cg20793665', 'cg23530553', 'cg24105081', 'cg25496181', 'cg25801976', 'cg26376241', 'rs1510189'], ['cg00532474', 'cg01208318', 'cg01252023', 'cg02239258', 'cg03672272', 'cg03956042', 'cg03972560', 'cg04857395', 'cg07195011', 'cg08698943', 'cg09232555', 'cg09462281', 'cg09975576', 'cg10473367', 'cg12057563', 'cg12448285', 'cg14371731', 'cg14381350', 'cg14499274', 'cg14531862', 'cg14584535', 'cg15288800', 'cg18384588', 'cg19537719', 'cg19893929', 'cg22731271', 'cg23528247', 'cg23690893', 'cg24675150', 'cg26235215', 'cg26607828'], ['cg00532474', 'cg01252023', 'cg02239258', 'cg03548415', 'cg03956042', 'cg03972560', 'cg04857395', 'cg05301188', 'cg07195011', 'cg08347500', 'cg08698943', 'cg08943714', 'cg09232555', 'cg09462281', 'cg09975576', 'cg10473367', 'cg12057563', 'cg12140851', 'cg12448285', 'cg13074055', 'cg14371731', 'cg14381350', 'cg14499274', 'cg14511923', 'cg14531862', 'cg15288800', 'cg15417249', 'cg16409562', 'cg18384588', 'cg19537719', 'cg19893929', 'cg21387009', 'cg21397540', 'cg22731271', 'cg23171972', 'cg23528247', 'cg23690893', 'cg23732024', 'cg24675150', 'cg25921609', 'cg26235215', 'cg26607828'], ['CT45A3.441519', 'cg01089498', 'cg01515515', 'cg02137956', 'cg05206633', 'cg05445326', 'cg05915866', 'cg06443533', 'cg07110356', 'cg07793808', 'cg09156207', 'cg09426383', 'cg09936008', 'cg10413105', 'cg10741025', 'cg10940462', 'cg11417025', 'cg12252069', 'cg12744859', 'cg13298116', 'cg13979887', 'cg14473743', 'cg17104258', 'cg18411150', 'cg18786623', 'cg19047660', 'cg20513976', 'cg20793665', 'cg22222281', 'cg23530553', 'cg23531049', 'cg25801976', 'cg26376241', 'hsa.mir.1248'], ['cg03967627', 'cg04655481', 'cg07816074', 'cg08943714', 'cg09232555', 'cg09462281', 'cg11227278', 'cg12057563', 'cg13005202', 'cg13073699', 'cg14381350', 'cg15219228', 'cg16409562', 'cg18179039', 'cg23732024', 'cg26607828', 'cg27214856'], ['cg01538731', 'cg05206633', 'cg06363129', 'cg09122035', 'cg12542656', 'cg13657981', 'cg14603098', 'cg14661139', 'cg14967804', 'cg23884076', 'cg25456368', 'cg25953130'], ['CT45A3.441519', 'cg00715696', 'cg01089498', 'cg01515515', 'cg02137956', 'cg03982355', 'cg05206633', 'cg05445326', 'cg05915866', 'cg06443533', 'cg06474225', 'cg07110356', 'cg07793808', 'cg08173263', 'cg09122035', 'cg09156207', 'cg09426383', 'cg09936008', 'cg10741025', 'cg10940462', 'cg11417025', 'cg12252069', 'cg12744859', 'cg13657981', 'cg13979887', 'cg14473743', 'cg17104258', 'cg17413460', 'cg18411150', 'cg18482303', 'cg18786623', 'cg19047660', 'cg20300129', 'cg20513976', 'cg20793665', 'cg22222281', 'cg23415434', 'cg23530553', 'cg23531049', 'cg24123198', 'cg25496181', 'cg25801976', 'cg26147845', 'cg26376241', 'hsa.mir.1248'], ['cg01542019', 'cg02340818', 'cg02367655', 'cg02680086', 'cg03309393', 'cg04656042', 'cg04858586', 'cg05856321', 'cg08447324', 'cg08525314', 'cg08604097', 'cg11342198', 'cg12499316', 'cg14511923', 'cg14603098', 'cg15070894', 'cg17534999', 'cg18179039', 'cg19966212', 'cg20359042', 'cg25145765', 'cg25953130', 'cg26607828', 'cg27072996'], ['cg01208318', 'cg02367655', 'cg03967627', 'cg08943714', 'cg09232555', 'cg09462281', 'cg10473367', 'cg12057563', 'cg12116137', 'cg12448285', 'cg13005202', 'cg14381350', 'cg14499274', 'cg14531862', 'cg15417249', 'cg15419294', 'cg18384588', 'cg21937377', 'cg24675150', 'cg26607828'], ['CT45A3.441519', 'cg00715696', 'cg00997251', 'cg01089498', 'cg01515515', 'cg01538731', 'cg02137956', 'cg03982355', 'cg05445326', 'cg05915866', 'cg06443533', 'cg06474225', 'cg07110356', 'cg07793808', 'cg08173263', 'cg08347500', 'cg09122035', 'cg09156207', 'cg09426383', 'cg09936008', 'cg10741025', 'cg10940462', 'cg12542656', 'cg12744859', 'cg13657981', 'cg13979887', 'cg14473743', 'cg17104258', 'cg17413460', 'cg18411150', 'cg18482303', 'cg18786623', 'cg20300129', 'cg20513976', 'cg22222281', 'cg23530553', 'cg23531049', 'cg24123198', 'cg25496181', 'cg25801976', 'cg26147845', 'cg26376241', 'hsa.mir.1248'], ['cg00532474', 'cg02891728', 'cg03173502', 'cg03604774', 'cg03972560', 'cg04857395', 'cg06283493', 'cg08447324', 'cg09232555', 'cg09975576', 'cg10473367', 'cg12057563', 'cg14371731', 'cg15090899', 'cg15417249', 'cg19893929', 'cg22731271', 'cg23171972', 'cg23690893', 'cg25145765', 'cg26607828'], ['cg00532474', 'cg00987534', 'cg01724917', 'cg02239258', 'cg03972560', 'cg03982355', 'cg04655481', 'cg04857395', 'cg05301188', 'cg05317090', 'cg05821046', 'cg07195011', 'cg08347500', 'cg08698943', 'cg08943714', 'cg09232555', 'cg09317239', 'cg09462281', 'cg10473367', 'cg10996816', 'cg11021222', 'cg11227278', 'cg12057563', 'cg12140851', 'cg12448285', 'cg12516875', 'cg13073699', 'cg13481969', 'cg14381350', 'cg14499274', 'cg14511923', 'cg14531862', 'cg15417249', 'cg16409562', 'cg18179039', 'cg18384588', 'cg18482303', 'cg19537719', 'cg19893929', 'cg19966212', 'cg21387009', 'cg21397540', 'cg22731271', 'cg23171972', 'cg23732024', 'cg24675150', 'cg26607828', 'cg27214856'], ['cg00791868', 'cg01252023', 'cg01724917', 'cg02239258', 'cg02891728', 'cg03173502', 'cg03548415', 'cg03956042', 'cg03972560', 'cg03982355', 'cg04655481', 'cg05317090', 'cg05821046', 'cg06466348', 'cg08347500', 'cg08447324', 'cg08943714', 'cg09232555', 'cg09462281', 'cg09975576', 'cg11021222', 'cg11227278', 'cg12057563', 'cg12448285', 'cg12516875', 'cg13005202', 'cg13073699', 'cg13936874', 'cg14381350', 'cg14511923', 'cg15288800', 'cg16409562', 'cg18179039', 'cg18384588', 'cg19537719', 'cg19893929', 'cg21397540', 'cg21637392', 'cg23690893', 'cg23732024', 'cg25591794', 'cg26235215', 'cg26607828', 'cg27214856', 'cg27285720'], ['cg00141845', 'cg00532474', 'cg00791868', 'cg03173502', 'cg03548415', 'cg03672272', 'cg03956042', 'cg05317090', 'cg05821046', 'cg07195011', 'cg08347500', 'cg08943714', 'cg09232555', 'cg09975576', 'cg10996816', 'cg12448285', 'cg12516875', 'cg14511923', 'cg14584535', 'cg16409562', 'cg19537719', 'cg19966212', 'cg21397540', 'cg23690893', 'cg23732024', 'cg26607828', 'cg27214856', 'cg27285720'], ['cg00791868', 'cg01252023', 'cg01542019', 'cg02605007', 'cg02891728', 'cg03407547', 'cg03548415', 'cg03672272', 'cg03956042', 'cg05317090', 'cg06466348', 'cg08447324', 'cg08943714', 'cg09232555', 'cg09975576', 'cg12516875', 'cg13073699', 'cg14084907', 'cg14381350', 'cg14684457', 'cg14703224', 'cg15288800', 'cg15916399', 'cg18179039', 'cg19317211', 'cg19537719', 'cg21637392', 'cg22365240', 'cg23690893', 'cg23732024', 'cg25591794', 'cg26607828', 'cg26814396', 'cg27214856', 'cg27285720'], ['cg01542019', 'cg02891728', 'cg03126946', 'cg03173502', 'cg03407547', 'cg03672272', 'cg03982355', 'cg06466348', 'cg08447324', 'cg08943714', 'cg09232555', 'cg09462281', 'cg12448285', 'cg13073699', 'cg14084907', 'cg14684457', 'cg14703224', 'cg18179039', 'cg26607828', 'cg26814396', 'cg27285720'], ['cg00861646', 'cg00987534', 'cg01208318', 'cg01231141', 'cg01542019', 'cg01724917', 'cg01891966', 'cg02239258', 'cg02367655', 'cg02605007', 'cg03967627', 'cg03972560', 'cg03982355', 'cg04655481', 'cg04857395', 'cg05301188', 'cg05317090', 'cg05821046', 'cg06466348', 'cg07195011', 'cg08347500', 'cg08698943', 'cg09232555', 'cg09317239', 'cg09462281', 'cg10473367', 'cg10996816', 'cg11021222', 'cg11227278', 'cg12057563', 'cg12140851', 'cg12516875', 'cg13073699', 'cg14018024', 'cg14084907', 'cg14200569', 'cg14684457', 'cg15417249', 'cg16409562', 'cg16651537', 'cg18179039', 'cg18384588', 'cg18482303', 'cg19317211', 'cg19537719', 'cg19893929', 'cg20927661', 'cg21387009', 'cg21397540', 'cg22731271', 'cg23171972', 'cg23732024', 'cg27214856', 'rs1510189'], ['SLC12A8.84561', 'cg06363129', 'cg08343075', 'cg15127250', 'cg15417249', 'cg17758673', 'cg19878482', 'cg19966212', 'cg24527636'], ['cg01724917', 'cg01891966', 'cg03604774', 'cg03982355', 'cg04797323', 'cg08347500', 'cg09462281', 'cg10980495', 'cg11021222', 'cg12448285', 'cg13936874', 'cg14511923', 'cg15417249', 'cg16306870', 'cg18482303', 'cg19537719', 'cg20747577', 'cg21397540', 'cg21637392', 'cg25020666', 'cg25145765', 'cg26235215'], ['CT45A3.441519', 'GPR142.350383', 'ZCCHC12.170261', 'cg01089498', 'cg01515515', 'cg02137956', 'cg05915866', 'cg06443533', 'cg07110356', 'cg08173263', 'cg09122035', 'cg09426383', 'cg10741025', 'cg10940462', 'cg11417025', 'cg12252069', 'cg12382846', 'cg12542656', 'cg12744859', 'cg13298116', 'cg13481969', 'cg13979887', 'cg14473743', 'cg16651537', 'cg17104258', 'cg17413460', 'cg18077307', 'cg18482303', 'cg18596381', 'cg18786623', 'cg20300129', 'cg20513976', 'cg20793665', 'cg21637392', 'cg22222281', 'cg23530553', 'cg24105081', 'cg25496181', 'cg25801976', 'cg26376241', 'cg26814396', 'rs1510189'], ['cg04268950', 'cg07127410', 'cg07816074', 'cg11204139', 'cg15241084', 'cg17216243', 'cg21375204', 'cg23840053', 'cg24199203', 'cg25685359', 'cg25884399'], ['cg01089498', 'cg02601475', 'cg03956042', 'cg05206633', 'cg05301188', 'cg05709437', 'cg05915866', 'cg06443533', 'cg07110356', 'cg08347500', 'cg09426383', 'cg11417025', 'cg12252069', 'cg12744859', 'cg13298116', 'cg13481969', 'cg13657981', 'cg13979887', 'cg17104258', 'cg18596381', 'cg18786623', 'cg20300129', 'cg23531049', 'cg24123198', 'cg25801976', 'cg26147845', 'cg26376241', 'hsa.mir.1248'], ['cg02367655', 'cg08698943', 'cg14511923', 'cg16651537', 'cg19317211', 'cg24199203', 'cg25884399'], ['cg02734358', 'cg09147140', 'cg12232731', 'cg13241645', 'cg15241084', 'cg21375204', 'cg23665778'], ['cg12382846', 'cg13481969', 'cg14661139', 'cg18413830', 'cg18596381', 'cg23042151', 'cg23415434'], ['cg00922748', 'cg00997251', 'cg01992935', 'cg02632490', 'cg02940147', 'cg03548415', 'cg03604774', 'cg03883572', 'cg07127410', 'cg08751352', 'cg09931909', 'cg11204139', 'cg13074055', 'cg13241645', 'cg14084907', 'cg14855841', 'cg15288800', 'cg16409562', 'cg16729415', 'cg17763019', 'cg19317211', 'cg20732787', 'cg23528247', 'cg23665778', 'cg26580869'], ['cg00610991', 'cg03883572', 'cg03972560', 'cg04268950', 'cg04857395', 'cg07127410', 'cg09975576', 'cg11204139', 'cg17763019', 'cg19893929', 'cg23690893', 'cg25145765', 'cg26580869'], ['CT45A3.441519', 'ZCCHC12.170261', 'cg00141845', 'cg00997251', 'cg02137956', 'cg03604774', 'cg05206633', 'cg05301188', 'cg05445326', 'cg05709437', 'cg06474225', 'cg06716182', 'cg07397033', 'cg07793808', 'cg08173263', 'cg08347500', 'cg08525314', 'cg08972170', 'cg09426383', 'cg09595185', 'cg09936008', 'cg10741025', 'cg10940462', 'cg11946459', 'cg12252069', 'cg12448285', 'cg12516875', 'cg13657981', 'cg13792579', 'cg14473743', 'cg14603098', 'cg15090899', 'cg17413460', 'cg18411150', 'cg20300129', 'cg20747577', 'cg20793665', 'cg21201401', 'cg23075364', 'cg24123198', 'cg25496181', 'cg25801976', 'cg26147845', 'cg27246129', 'hsa.mir.1248'], ['cg00922748', 'cg00997251', 'cg01252023', 'cg01992935', 'cg02239258', 'cg02632490', 'cg02940147', 'cg03604774', 'cg03883572', 'cg03956042', 'cg04268950', 'cg04797323', 'cg06635946', 'cg07127410', 'cg07195011', 'cg07397033', 'cg08183317', 'cg08525314', 'cg08751352', 'cg08943714', 'cg09931909', 'cg10980495', 'cg11204139', 'cg11227278', 'cg12448285', 'cg13074055', 'cg14084907', 'cg14511923', 'cg14855841', 'cg15070894', 'cg15234492', 'cg15288800', 'cg16409562', 'cg16651537', 'cg16729415', 'cg17763019', 'cg18482303', 'cg19317211', 'cg21201401', 'cg21637392', 'cg23171972', 'cg26580869'], ['cg00715696', 'cg05915866', 'cg06443533', 'cg07110356', 'cg12835118', 'cg17104258'], ['cg01252023', 'cg03548415', 'cg03972560', 'cg04268950', 'cg07127410', 'cg07195011', 'cg08183317', 'cg08196968', 'cg08751352', 'cg09232555', 'cg09462281', 'cg09931909', 'cg10473367', 'cg11204139', 'cg11977716', 'cg12499316', 'cg13006424', 'cg15288800', 'cg16729415', 'cg19537719', 'cg19893929', 'cg21397540', 'cg21637392', 'cg22365240', 'cg23528247', 'cg25591794', 'cg26235215', 'cg26580869', 'hsa.mir.153.2'], ['CA4.762', 'IGSF5.150084', 'SHANK1.50944', 'SIX3.6496', 'SLC35F4.341880', 'cg00532474', 'cg01423695', 'cg01517680', 'cg03060555', 'cg03672272', 'cg04821520', 'cg05006231', 'cg05830220', 'cg05915866', 'cg06443533', 'cg12079322', 'cg12382846', 'cg12744859', 'cg13298116', 'cg15085883', 'cg15145341', 'cg18786623', 'cg20518446', 'cg21805940'], ['C3.718', 'GLDN.342035', 'cg00715696', 'cg01089498', 'cg01538731', 'cg02137956', 'cg02239258', 'cg04287574', 'cg04821520', 'cg06716182', 'cg06869935', 'cg07397033', 'cg07793808', 'cg08173263', 'cg09156207', 'cg09462281', 'cg09936008', 'cg10136008', 'cg12079322', 'cg12232731', 'cg12252069', 'cg12444411', 'cg12516875', 'cg12542656', 'cg13657981', 'cg13792579', 'cg14473743', 'cg14603098', 'cg16658931', 'cg17413460', 'cg18077307', 'cg18349022', 'cg18411150', 'cg20793665', 'cg21201401', 'cg22198603', 'cg23531049', 'cg23762517', 'cg24123198', 'cg24862510', 'cg25496181', 'cg25605731', 'cg25801976', 'cg27246129', 'hsa.mir.1248', 'hsa.mir.155'], ['ATP10B.23120', 'CA4.762', 'PISRT1.140464', 'SHANK1.50944', 'SIX3.6496', 'SYCE1.93426', 'TM7SF4.81501', 'UGT2B7.7364', 'cg01231141', 'cg01423695', 'cg01515515', 'cg01517680', 'cg02556928', 'cg03672272', 'cg03967627', 'cg05006231', 'cg05830220', 'cg05915866', 'cg06443533', 'cg07110356', 'cg07557423', 'cg07971820', 'cg08928675', 'cg10146935', 'cg10413105', 'cg10566121', 'cg10940462', 'cg12079322', 'cg12116137', 'cg12382846', 'cg12744859', 'cg13241645', 'cg13298116', 'cg13481969', 'cg14425733', 'cg14531862', 'cg15085883', 'cg15145341', 'cg15419294', 'cg16306870', 'cg17104258', 'cg18077307', 'cg18596381', 'cg18786623', 'cg19047660', 'cg20910008', 'cg23075364', 'cg23665778', 'cg24757160', 'cg26376241', 'cg26814396', 'cg27246129'], ['cg00141845', 'cg01252023', 'cg01542019', 'cg01724917', 'cg02891728', 'cg03060555', 'cg03254465', 'cg03407547', 'cg03604774', 'cg03672272', 'cg05301188', 'cg06283493', 'cg06466348', 'cg06635946', 'cg07816074', 'cg07971820', 'cg08183317', 'cg09113070', 'cg09294072', 'cg10136008', 'cg10980495', 'cg10996816', 'cg11021222', 'cg11227278', 'cg12382846', 'cg12448285', 'cg13481969', 'cg14371731', 'cg14855841', 'cg15234492', 'cg18482303', 'cg18663063', 'cg19966212', 'cg20732787', 'cg21397540', 'cg23075364', 'cg23422268', 'cg23665778', 'cg24163242', 'cg25145765', 'cg26607828', 'cg27280688', 'cg27285720'], ['cg03564727', 'cg05551825', 'cg09122035', 'cg14171514', 'cg20513976'], ['cg04858586', 'cg14603098', 'cg27072996', 'cg27404676'], ['cg07397033', 'cg07816074', 'cg15241084', 'cg15916399'], ['cg02734358', 'cg11697194', 'cg12232731', 'cg26806779'], ['cg14661139', 'cg14967804', 'cg23884076'], ['cg01231141', 'cg13241645', 'cg23665778'], ['cg08698943', 'cg22731271', 'cg23171972'], ['CHAC1.79094', 'CYP17A1.1586', 'FAM170B.170370', 'LPO.4025', 'MYBPH.4608', 'PISRT1.140464', 'S100A9.6280', 'SHANK1.50944', 'ST8SIA1.6489', 'TNNI2.7136', 'cg00141845', 'cg00610991', 'cg00715696', 'cg00861646', 'cg00863893', 'cg00980649', 'cg01089498', 'cg01253160', 'cg01515515', 'cg01538731', 'cg01642895', 'cg01824603', 'cg02137956', 'cg02403395', 'cg02556928', 'cg02605007', 'cg03173502', 'cg03240511', 'cg03604774', 'cg03672272', 'cg04821520', 'cg05445326', 'cg05915866', 'cg06040034', 'cg06443533', 'cg06474225', 'cg06716182', 'cg07110356', 'cg07195011', 'cg07793808', 'cg08173263', 'cg08347500', 'cg08698943', 'cg08972170', 'cg09426383', 'cg09556515', 'cg09595185', 'cg10146935', 'cg10413105', 'cg10741025', 'cg10940462', 'cg11308319', 'cg11946459', 'cg12252069', 'cg12448285', 'cg12516875', 'cg12744859', 'cg13420985', 'cg13657981', 'cg13792579', 'cg14084907', 'cg14473743', 'cg14603098', 'cg15090899', 'cg17104258', 'cg17413460', 'cg17942750', 'cg18596381', 'cg18786623', 'cg20076659', 'cg20300129', 'cg20359042', 'cg20513976', 'cg21201401', 'cg21419585', 'cg23075364', 'cg23325963', 'cg23415434', 'cg23553912', 'cg24123198', 'cg24422489', 'cg24742520', 'cg25496181', 'cg25801976', 'cg25988214', 'cg26147845', 'cg26376241', 'cg26607828', 'cg27246129', 'cg27285720', 'hsa.mir.100', 'hsa.mir.1248', 'hsa.mir.181c'], ['ACTL9.284382', 'AR.367', 'ATP12A.479', 'BCL8.606', 'CASP5.838', 'CXADRP3.440224', 'ETV7.51513', 'GPR142.350383', 'HIST1H3B.8358', 'HRASLS.57110', 'MTTP.4547', 'MYBPH.4608', 'MYOF.26509', 'NPY.4852', 'PCDHGB8P.56120', 'PF4.5196', 'SCRT2.85508', 'SELV.348303', 'SORCS2.57537', 'TM7SF4.81501', 'TMPRSS11D.9407', 'ZCCHC12.170261', 'cg00715696', 'cg00861646', 'cg01515515', 'cg02137956', 'cg02556928', 'cg03564727', 'cg04797323', 'cg05830220', 'cg05915866', 'cg06022561', 'cg06313676', 'cg06443533', 'cg06943420', 'cg07110356', 'cg07355926', 'cg07793808', 'cg09113070', 'cg09426383', 'cg10898024', 'cg10940462', 'cg12744859', 'cg13979887', 'cg14115756', 'cg14473743', 'cg14661139', 'cg15361750', 'cg16006841', 'cg16200531', 'cg17104258', 'cg17643598', 'cg18077307', 'cg18413830', 'cg18786623', 'cg20300129', 'cg20793665', 'cg21055528', 'cg22222281', 'cg23270529', 'cg23530553', 'cg25197194', 'cg27285720', 'rs5936512', 'hsa.mir.1248', 'hsa.mir.154', 'hsa.mir.155'], ['AREG.374', 'BARHL1.56751', 'C14orf165.414767', 'CAPN6.827', 'CCL22.6367', 'CDH5.1003', 'DLK1.8788', 'FAM157B.100132403', 'FLJ34503.285759', 'FLJ39609.100130417', 'FXYD2.486', 'GPR152.390212', 'HIST1H3B.8358', 'IRF6.3664', 'KCNE1.3753', 'KIAA0087.9808', 'LCE1E.353135', 'LOC284632.284632', 'LYG2.254773', 'MB.4151', 'MTTP.4547', 'MYBPH.4608', 'PCDHB3.56132', 'PCDHB4.56131', 'PHACTR3.116154', 'POPDC3.64208', 'POU4F2.5458', 'SHANK1.50944', 'SLC35F4.341880', 'TNNI2.7136', 'VNN1.8876', 'cg00044505', 'cg02734358', 'cg07588875', 'cg10566121', 'cg11697194', 'cg11949518', 'cg15085883', 'cg16366262', 'cg21822187', 'cg22151446', 'cg23042151', 'cg26806779', 'cg26880735', 'hsa.mir.142', 'hsa.mir.153.2', 'hsa.mir.376c', 'hsa.mir.665'], ['C14orf165.414767', 'CCL26.10344', 'CLCA4.22802', 'DUSP5P.574029', 'FAM157B.100132403', 'HRH4.59340', 'KIAA0087.9808', 'MCF2L.23263', 'OR7A5.26659', 'PLCB4.5332', 'RGMA.56963', 'VANGL2.57216', 'cg00922748', 'cg02239258', 'cg02505827', 'cg02734358', 'cg02940147', 'cg05304979', 'cg06538334', 'cg06869935', 'cg07588875', 'cg09147140', 'cg09667606', 'cg09704168', 'cg10536999', 'cg10566121', 'cg11876048', 'cg12140851', 'cg12232731', 'cg12360736', 'cg15916399', 'cg16006841', 'cg16366262', 'cg16651537', 'cg17216243', 'cg17763019', 'cg20910008', 'cg20927661', 'cg22151446', 'cg22157027', 'cg23248424', 'cg23530553', 'cg24123198', 'cg24790297', 'hsa.let.7a.1', 'hsa.mir.588']]\n",
      "Data: [[['24:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['69:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['54:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['11:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['74:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['90:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['79:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['129:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['49:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['84:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['32:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['104:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['36:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['116:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['25:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['119:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['130:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['99:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['110:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['51:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1']], [['13:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['129:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['90:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['69:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['54:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['49:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['11:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['74:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['79:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['84:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['24:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['32:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['104:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['116:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['36:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1']], [['10:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['19:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['138:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['72:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['109:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['37:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['95:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['128:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['153:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['78:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['2:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['113:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['117:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['103:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['27:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']], [['120:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['145:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['57:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['49:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['69:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['54:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['90:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['129:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['84:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['119:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['36:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['24:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['51:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['104:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['116:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1']], [['27:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['50:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['19:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['37:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['72:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['138:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['109:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['78:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['95:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['113:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['2:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['103:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['117:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['153:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']], [['19:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['37:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['72:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['138:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['109:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['78:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['95:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['113:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['128:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['153:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['2:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['117:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['103:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']], [['18:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['129:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['69:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['90:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['49:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['54:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['11:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['79:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['84:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['74:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['119:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['99:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['116:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1']], [['67:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['112:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['132:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['37:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['19:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['109:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['78:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['138:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['72:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['95:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['23:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['62:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['113:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']], [['39:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['123:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['28:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['47:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['92:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['102:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['107:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['105:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['26:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['68:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['111:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['58:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['0:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['7:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['55:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['93:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['44:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['41:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['22:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['133:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['45:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['137:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['155:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1'], ['82:', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '-1']], [['69:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['129:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['54:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['49:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['90:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['11:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['79:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['74:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['84:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['104:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['116:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['99:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1']], [['7:', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['65:', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['21:', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['88:', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['55:', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['121:', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['60:', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['91:', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['64:', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['146:', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['44:', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['93:', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1']], [['112:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['132:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['78:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['19:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['109:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['37:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['138:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['72:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['95:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['27:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['103:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['117:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']], [['48:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['69:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['11:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['90:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['129:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['54:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['74:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['79:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['84:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['49:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['104:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['116:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1']], [['113:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['157:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['95:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['138:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['19:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['109:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['37:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['72:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['128:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['2:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['117:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['27:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']], [['37:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['112:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['19:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['78:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['109:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['72:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['138:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['95:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['128:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['153:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['113:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']], [['23:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['37:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['19:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['109:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['72:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['78:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['138:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['95:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['113:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['62:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['128:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']], [['95:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['126:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['113:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['138:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['19:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['109:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['37:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['72:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['78:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['23:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['153:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']], [['14:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['95:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['19:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['109:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['78:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['37:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['138:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['72:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['23:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['62:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']], [['19:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['75:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['95:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['138:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['72:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['78:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['37:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['109:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['23:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['62:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']], [['20:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['112:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['37:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['78:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['109:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['19:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['72:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['138:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['95:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']], [['47:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['142:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['28:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['123:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['92:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['105:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['39:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['68:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['26:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['102:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['58:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['0:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['107:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['108:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['111:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['88:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['7:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['21:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['93:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1']], [['62:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['94:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['152:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['3:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['2:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['113:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['73:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['128:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']], [['69:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['120:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['129:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['49:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['119:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['54:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['90:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['84:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1']], [['127:', '1', '1', '-1', '1', '-1', '-1', '-1', '-1', '1', '-1', '1'], ['154:', '1', '1', '-1', '1', '-1', '-1', '-1', '-1', '1', '-1', '1'], ['131:', '1', '1', '-1', '1', '-1', '-1', '-1', '-1', '1', '-1', '1'], ['143:', '1', '1', '-1', '1', '-1', '-1', '-1', '-1', '1', '-1', '1'], ['34:', '1', '1', '-1', '1', '-1', '-1', '-1', '-1', '1', '-1', '1'], ['3:', '1', '1', '-1', '1', '-1', '-1', '-1', '-1', '1', '-1', '1'], ['73:', '1', '1', '-1', '1', '-1', '-1', '-1', '-1', '1', '-1', '1'], ['53:', '1', '1', '-1', '1', '-1', '-1', '-1', '-1', '1', '-1', '1']], [['69:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['118:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['79:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['11:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['49:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['90:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['54:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['84:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1']], [['19:', '1', '1', '1', '1', '1', '1', '1'], ['156:', '1', '1', '1', '1', '1', '1', '1'], ['78:', '1', '1', '1', '1', '1', '1', '1'], ['72:', '1', '1', '1', '1', '1', '1', '1'], ['109:', '1', '1', '1', '1', '1', '1', '1'], ['37:', '1', '1', '1', '1', '1', '1', '1'], ['138:', '1', '1', '1', '1', '1', '1', '1'], ['95:', '1', '1', '1', '1', '1', '1', '1'], ['2:', '1', '1', '1', '1', '1', '1', '1'], ['128:', '1', '1', '1', '1', '1', '1', '1'], ['153:', '1', '1', '1', '1', '1', '1', '1'], ['103:', '1', '1', '1', '1', '1', '1', '1'], ['62:', '1', '1', '1', '1', '1', '1', '1'], ['112:', '1', '1', '1', '1', '1', '1', '1'], ['127:', '1', '1', '1', '1', '1', '1', '1'], ['73:', '1', '1', '1', '1', '1', '1', '1'], ['122:', '1', '1', '1', '1', '1', '1', '1'], ['46:', '1', '1', '1', '1', '1', '1', '1'], ['3:', '1', '1', '1', '1', '1', '1', '1'], ['94:', '1', '1', '1', '1', '1', '1', '1'], ['64:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['84:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['88:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['91:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['121:', '-1', '-1', '-1', '-1', '-1', '-1', '-1']], [['131:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['149:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['143:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['59:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['12:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['73:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['97:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['35:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['101:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['63:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['16:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['87:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['80:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['53:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['127:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['100:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['158:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['17:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['18:', '-1', '-1', '-1', '-1', '-1', '-1', '-1']], [['69:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['115:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['49:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['129:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['54:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['79:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['8:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['11:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['90:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['18:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['57:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['84:', '-1', '-1', '-1', '-1', '-1', '-1', '-1'], ['27:', '1', '1', '1', '1', '1', '1', '1'], ['95:', '1', '1', '1', '1', '1', '1', '1']], [['96:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '-1', '1'], ['127:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '-1', '1'], ['131:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '-1', '1'], ['2:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '-1', '1'], ['46:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '-1', '1'], ['153:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '-1', '1'], ['103:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '-1', '1']], [['46:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['147:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['128:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['117:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['50:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['2:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['62:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']], [['1:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['32:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['69:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['11:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['74:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1'], ['24:', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1']], [['131:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['150:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['127:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['153:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['128:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['2:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']], [['4:', '1', '1', '1', '1', '1', '1'], ['45:', '1', '1', '1', '1', '1', '1'], ['82:', '1', '1', '1', '1', '1', '1'], ['7:', '1', '1', '1', '1', '1', '1'], ['133:', '1', '1', '1', '1', '1', '1'], ['155:', '1', '1', '1', '1', '1', '1'], ['71:', '1', '1', '1', '1', '1', '1'], ['21:', '1', '1', '1', '1', '1', '1'], ['39:', '1', '1', '1', '1', '1', '1'], ['94:', '1', '1', '1', '1', '1', '1'], ['111:', '1', '1', '1', '1', '1', '1'], ['58:', '1', '1', '1', '1', '1', '1'], ['14:', '1', '1', '1', '1', '1', '1'], ['62:', '1', '1', '1', '1', '1', '1'], ['152:', '1', '1', '1', '1', '1', '1'], ['3:', '1', '1', '1', '1', '1', '1'], ['27:', '1', '1', '1', '1', '1', '1'], ['50:', '1', '1', '1', '1', '1', '1'], ['63:', '1', '1', '1', '1', '1', '1'], ['65:', '1', '1', '1', '1', '1', '1'], ['68:', '1', '1', '1', '1', '1', '1'], ['107:', '1', '1', '1', '1', '1', '1']], [['6:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['63:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['128:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['153:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['2:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['50:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']], [['134:', '-1', '-1', '1', '-1', '1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1'], ['136:', '-1', '-1', '1', '-1', '1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1'], ['18:', '-1', '-1', '1', '-1', '1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1'], ['8:', '-1', '-1', '1', '-1', '1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1'], ['29:', '-1', '-1', '1', '-1', '1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1'], ['66:', '-1', '-1', '1', '-1', '1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1']], [['15:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '1', '-1', '-2', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '1', '-1', '-1', '1', '-1'], ['32:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '1', '-1', '-2', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '1', '-1', '-1', '1', '-1'], ['141:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '1', '-1', '-2', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '1', '-1', '-1', '1', '-1'], ['104:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '1', '-1', '-2', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '1', '-1', '-1', '1', '-1'], ['144:', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '1', '-1', '-2', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '-1', '-1', '1', '-1', '-1', '1', '-1']], [['18:', '-2', '-1', '1', '1', '-1', '1', '1', '-2', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1'], ['29:', '-2', '-1', '1', '1', '-1', '1', '1', '-2', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1'], ['134:', '-2', '-1', '1', '1', '-1', '1', '1', '-2', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1'], ['8:', '-2', '-1', '1', '1', '-1', '1', '1', '-2', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1'], ['49:', '-2', '-1', '1', '1', '-1', '1', '1', '-2', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1']], [['30:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['113:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['95:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['138:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'], ['19:', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']], [['3:', '1', '1', '1', '1', '1'], ['33:', '1', '1', '1', '1', '1'], ['133:', '1', '1', '1', '1', '1'], ['27:', '1', '1', '1', '1', '1'], ['28:', '1', '1', '1', '1', '1'], ['92:', '1', '1', '1', '1', '1'], ['47:', '1', '1', '1', '1', '1'], ['0:', '1', '1', '1', '1', '1'], ['58:', '1', '1', '1', '1', '1'], ['123:', '1', '1', '1', '1', '1'], ['68:', '1', '1', '1', '1', '1'], ['26:', '1', '1', '1', '1', '1'], ['102:', '1', '1', '1', '1', '1'], ['39:', '1', '1', '1', '1', '1'], ['105:', '1', '1', '1', '1', '1'], ['82:', '1', '1', '1', '1', '1'], ['107:', '1', '1', '1', '1', '1'], ['111:', '1', '1', '1', '1', '1'], ['45:', '1', '1', '1', '1', '1'], ['7:', '1', '1', '1', '1', '1']], [['7:', '-1', '1', '-1', '-1'], ['21:', '-1', '1', '-1', '-1'], ['55:', '-1', '1', '-1', '-1'], ['93:', '-1', '1', '-1', '-1'], ['44:', '-1', '1', '-1', '-1'], ['102:', '-1', '1', '-1', '-1'], ['123:', '-1', '1', '-1', '-1'], ['28:', '-1', '1', '-1', '-1'], ['107:', '-1', '1', '-1', '-1'], ['92:', '-1', '1', '-1', '-1'], ['26:', '-1', '1', '-1', '-1'], ['47:', '-1', '1', '-1', '-1'], ['105:', '-1', '1', '-1', '-1'], ['39:', '-1', '1', '-1', '-1'], ['68:', '-1', '1', '-1', '-1'], ['58:', '-1', '1', '-1', '-1'], ['111:', '-1', '1', '-1', '-1'], ['121:', '-1', '1', '-1', '-1'], ['64:', '-1', '1', '-1', '-1'], ['124:', '-1', '1', '-1', '-1'], ['41:', '-1', '1', '-1', '-1'], ['81:', '-1', '1', '-1', '-1'], ['60:', '-1', '1', '-1', '-1'], ['146:', '-1', '1', '-1', '-1'], ['22:', '-1', '1', '-1', '-1'], ['71:', '-1', '1', '-1', '-1'], ['45:', '-1', '1', '-1', '-1'], ['137:', '-1', '1', '-1', '-1'], ['82:', '-1', '1', '-1', '-1'], ['83:', '-1', '1', '-1', '-1'], ['91:', '-1', '1', '-1', '-1'], ['88:', '-1', '1', '-1', '-1'], ['114:', '-1', '1', '-1', '-1'], ['140:', '-1', '1', '-1', '-1'], ['27:', '-1', '1', '-1', '-1'], ['98:', '-1', '1', '-1', '-1'], ['66:', '-1', '1', '-1', '-1'], ['155:', '-1', '1', '-1', '-1']], [['56:', '1', '-1', '-1', '-1'], ['61:', '1', '-1', '-1', '-1'], ['43:', '1', '-1', '-1', '-1'], ['97:', '1', '-1', '-1', '-1'], ['66:', '1', '-1', '-1', '-1'], ['86:', '1', '-1', '-1', '-1'], ['125:', '1', '-1', '-1', '-1'], ['34:', '1', '-1', '-1', '-1'], ['100:', '1', '-1', '-1', '-1'], ['127:', '1', '-1', '-1', '-1'], ['131:', '1', '-1', '-1', '-1'], ['143:', '1', '-1', '-1', '-1'], ['31:', '1', '-1', '-1', '-1'], ['52:', '1', '-1', '-1', '-1'], ['12:', '1', '-1', '-1', '-1'], ['57:', '1', '-1', '-1', '-1'], ['59:', '1', '-1', '-1', '-1'], ['70:', '1', '-1', '-1', '-1'], ['77:', '1', '-1', '-1', '-1'], ['80:', '1', '-1', '-1', '-1'], ['101:', '1', '-1', '-1', '-1'], ['158:', '1', '-1', '-1', '-1'], ['82:', '-1', '1', '1', '1']], [['42:', '-1', '-1', '-1', '-1'], ['127:', '-1', '-1', '-1', '-1'], ['131:', '-1', '-1', '-1', '-1'], ['76:', '-1', '-1', '-1', '-1'], ['53:', '-1', '-1', '-1', '-1'], ['12:', '-1', '-1', '-1', '-1'], ['73:', '-1', '-1', '-1', '-1'], ['97:', '-1', '-1', '-1', '-1'], ['59:', '-1', '-1', '-1', '-1'], ['101:', '-1', '-1', '-1', '-1'], ['87:', '-1', '-1', '-1', '-1'], ['80:', '-1', '-1', '-1', '-1'], ['143:', '-1', '-1', '-1', '-1'], ['100:', '-1', '-1', '-1', '-1'], ['148:', '-1', '-1', '-1', '-1'], ['17:', '-1', '-1', '-1', '-1'], ['18:', '-1', '-1', '-1', '-1'], ['89:', '-1', '-1', '-1', '-1'], ['31:', '-1', '-1', '-1', '-1'], ['5:', '-1', '-1', '-1', '-1'], ['9:', '-1', '-1', '-1', '-1'], ['52:', '-1', '-1', '-1', '-1'], ['77:', '-1', '-1', '-1', '-1'], ['85:', '-1', '-1', '-1', '-1'], ['153:', '-1', '-1', '-1', '-1'], ['19:', '1', '1', '1', '1'], ['72:', '1', '1', '1', '1']], [['39:', '1', '1', '1'], ['108:', '1', '1', '1'], ['68:', '1', '1', '1'], ['58:', '1', '1', '1'], ['47:', '1', '1', '1'], ['28:', '1', '1', '1'], ['123:', '1', '1', '1'], ['0:', '1', '1', '1'], ['92:', '1', '1', '1'], ['105:', '1', '1', '1'], ['26:', '1', '1', '1'], ['102:', '1', '1', '1'], ['107:', '1', '1', '1'], ['111:', '1', '1', '1'], ['93:', '1', '1', '1'], ['7:', '1', '1', '1'], ['44:', '1', '1', '1'], ['45:', '1', '1', '1'], ['133:', '1', '1', '1'], ['137:', '1', '1', '1'], ['94:', '1', '1', '1'], ['82:', '1', '1', '1'], ['155:', '1', '1', '1'], ['55:', '1', '1', '1'], ['41:', '1', '1', '1'], ['22:', '1', '1', '1'], ['27:', '1', '1', '1'], ['140:', '1', '1', '1'], ['3:', '1', '1', '1']], [['76:', '-1', '-1', '-1'], ['131:', '-1', '-1', '-1'], ['127:', '-1', '-1', '-1'], ['34:', '-1', '-1', '-1'], ['63:', '-1', '-1', '-1'], ['153:', '-1', '-1', '-1'], ['2:', '-1', '-1', '-1'], ['62:', '-1', '-1', '-1'], ['103:', '-1', '-1', '-1'], ['46:', '-1', '-1', '-1'], ['122:', '-1', '-1', '-1'], ['59:', '-1', '-1', '-1'], ['101:', '-1', '-1', '-1'], ['73:', '-1', '-1', '-1'], ['35:', '-1', '-1', '-1'], ['53:', '-1', '-1', '-1'], ['12:', '-1', '-1', '-1'], ['43:', '-1', '-1', '-1'], ['97:', '-1', '-1', '-1'], ['148:', '-1', '-1', '-1'], ['8:', '-1', '-1', '-1'], ['17:', '-1', '-1', '-1'], ['18:', '-1', '-1', '-1'], ['29:', '-1', '-1', '-1'], ['49:', '-1', '-1', '-1'], ['69:', '-1', '-1', '-1'], ['70:', '-1', '-1', '-1'], ['79:', '-1', '-1', '-1'], ['80:', '-1', '-1', '-1'], ['87:', '-1', '-1', '-1'], ['100:', '-1', '-1', '-1'], ['134:', '-1', '-1', '-1'], ['151:', '-1', '-1', '-1'], ['158:', '-1', '-1', '-1'], ['37:', '1', '1', '1']], [['16:', '1', '1', '1'], ['63:', '1', '1', '1'], ['153:', '1', '1', '1'], ['128:', '1', '1', '1'], ['2:', '1', '1', '1'], ['46:', '1', '1', '1'], ['122:', '1', '1', '1'], ['103:', '1', '1', '1'], ['10:', '1', '1', '1'], ['56:', '1', '1', '1'], ['19:', '1', '1', '1'], ['37:', '1', '1', '1'], ['95:', '1', '1', '1'], ['109:', '1', '1', '1'], ['113:', '1', '1', '1'], ['138:', '1', '1', '1'], ['72:', '1', '1', '1'], ['20:', '1', '1', '1'], ['112:', '1', '1', '1'], ['62:', '1', '1', '1'], ['78:', '1', '1', '1'], ['117:', '1', '1', '1'], ['125:', '1', '1', '1'], ['94:', '1', '1', '1'], ['27:', '1', '1', '1'], ['38:', '1', '1', '1'], ['143:', '1', '1', '1'], ['7:', '-1', '-1', '-1'], ['11:', '-1', '-1', '-1'], ['21:', '-1', '-1', '-1'], ['54:', '-1', '-1', '-1'], ['64:', '-1', '-1', '-1'], ['69:', '-1', '-1', '-1'], ['79:', '-1', '-1', '-1'], ['84:', '-1', '-1', '-1'], ['88:', '-1', '-1', '-1'], ['90:', '-1', '-1', '-1'], ['102:', '-1', '-1', '-1'], ['129:', '-1', '-1', '-1']], [['40:', '1', '-1', '-2', '-1', '1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-2', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '1'], ['74:', '1', '-1', '-2', '-1', '1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-2', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '1'], ['11:', '1', '-1', '-2', '-1', '1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-2', '-1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '1']], [['49:', '-1', '1', '-2', '-1', '1', '1', '-1', '-1', '-1', '1', '1', '1', '1', '1', '-1', '1', '1', '1', '-3', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '1', '1', '-1'], ['106:', '-1', '1', '-2', '-1', '1', '1', '-1', '-1', '-1', '1', '1', '1', '1', '1', '-1', '1', '1', '1', '-3', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '1', '1', '-1'], ['129:', '-1', '1', '-2', '-1', '1', '1', '-1', '-1', '-1', '1', '1', '1', '1', '1', '-1', '1', '1', '1', '-3', '1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '1', '1', '-1']], [['5:', '1', '1', '1', '2', '1', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '1', '1', '1', '1', '2', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '1'], ['139:', '1', '1', '1', '2', '1', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '1', '1', '1', '1', '2', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '1'], ['53:', '1', '1', '1', '2', '1', '2', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '1', '1', '1', '1', '2', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '-1', '1', '1', '1']], [['135:', '1', '-1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '1', '1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '1', '1', '-1', '1', '1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '1', '1', '1', '1', '1'], ['143:', '1', '-1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '1', '1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '1', '1', '-1', '1', '1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '1', '1', '1', '1', '1'], ['131:', '1', '-1', '1', '-1', '1', '1', '1', '1', '1', '1', '1', '-1', '1', '1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '1', '1', '-1', '1', '1', '-1', '-1', '-1', '-1', '-1', '1', '-1', '1', '-1', '-1', '-1', '-1', '-1', '1', '1', '1', '1', '1']]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize variables to store data\n",
    "blocks = []\n",
    "genes = []\n",
    "conditions = []\n",
    "data = []\n",
    "\n",
    "# Read the file line by line\n",
    "with open('qubic/filtered_features_index.tsv.blocks', 'r') as file:\n",
    "    current_block = None\n",
    "    current_genes = []\n",
    "    current_conditions = []\n",
    "    current_data = []\n",
    "\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Skip comment lines\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "\n",
    "        # Check for a new block\n",
    "        if line.startswith('BC'):\n",
    "            if current_block is not None:\n",
    "                # Save the previous block's data\n",
    "                blocks.append(current_block)\n",
    "                genes.append(current_genes)\n",
    "                conditions.append(current_conditions)\n",
    "                data.append(current_data)\n",
    "\n",
    "            # Start a new block\n",
    "            current_block = line\n",
    "            current_genes = []\n",
    "            current_conditions = []\n",
    "            current_data = []\n",
    "\n",
    "        # Extract genes\n",
    "        elif line.startswith('Genes'):\n",
    "            current_genes = line.split(':')[1].strip().split()\n",
    "\n",
    "        # Extract conditions\n",
    "        elif line.startswith('Conds'):\n",
    "            current_conditions = line.split(':')[1].strip().split()\n",
    "\n",
    "        # Extract data (numeric values)\n",
    "        elif line and not line.startswith('BC'):\n",
    "            current_data.append(line.split())\n",
    "\n",
    "    # Save the last block's data\n",
    "    if current_block is not None:\n",
    "        blocks.append(current_block)\n",
    "        genes.append(current_genes)\n",
    "        conditions.append(current_conditions)\n",
    "        data.append(current_data)\n",
    "\n",
    "# Print the extracted data for inspection\n",
    "print(\"Blocks:\", blocks)\n",
    "print(\"Genes:\", genes)\n",
    "print(\"Conditions:\", conditions)\n",
    "print(\"Data:\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Mismatch in columns. Data has 24 columns, but conditions has 23.\n",
      "Truncating or padding the data to match the number of conditions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 9 (\t) missing from font(s) Arial.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Create a heatmap for the first block\n",
    "block_index = 0  # Change this to visualize a different block\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "block_data = pd.DataFrame(data[block_index])\n",
    "\n",
    "# Check the number of columns in the data\n",
    "num_columns = len(block_data.columns)\n",
    "\n",
    "# Check the number of conditions\n",
    "num_conditions = len(conditions[block_index])\n",
    "\n",
    "# If there's a mismatch, fix it\n",
    "if num_columns != num_conditions:\n",
    "    print(f\"Warning: Mismatch in columns. Data has {num_columns} columns, but conditions has {num_conditions}.\")\n",
    "    print(\"Truncating or padding the data to match the number of conditions.\")\n",
    "\n",
    "    # Truncate or pad the data to match the number of conditions\n",
    "    if num_columns > num_conditions:\n",
    "        block_data = block_data.iloc[:, :num_conditions]  # Truncate extra columns\n",
    "    else:\n",
    "        # Pad missing columns with NaN\n",
    "        for _ in range(num_conditions - num_columns):\n",
    "            block_data[num_columns + _] = None\n",
    "\n",
    "# Set the column names to the conditions\n",
    "block_data.columns = conditions[block_index]\n",
    "\n",
    "# Set the row names to the genes\n",
    "block_data.index = genes[block_index]\n",
    "\n",
    "# Convert the DataFrame to numeric (if necessary)\n",
    "block_data = block_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(block_data, cmap='viridis', annot=True, fmt=\".2f\")\n",
    "plt.title(f'Heatmap for Block {blocks[block_index]}')\n",
    "plt.xlabel('Conditions')\n",
    "plt.ylabel('Genes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block BC000\tS=460\tEnrichment:1.94\tRow=20\tCol=23\tCore_Row=20\tCore_Col=23: 20 genes x 23 conditions\n",
      "Block BC001\tS=480\tEnrichment:1.93\tRow=15\tCol=32\tCore_Row=15\tCore_Col=32: 15 genes x 32 conditions\n",
      "Block BC002\tS=435\tEnrichment:1.91\tRow=15\tCol=29\tCore_Row=15\tCore_Col=29: 15 genes x 29 conditions\n",
      "Block BC003\tS=270\tEnrichment:1.97\tRow=15\tCol=18\tCore_Row=15\tCore_Col=18: 15 genes x 18 conditions\n",
      "Block BC004\tS=434\tEnrichment:1.92\tRow=14\tCol=31\tCore_Row=14\tCore_Col=31: 14 genes x 31 conditions\n",
      "Block BC005\tS=546\tEnrichment:1.94\tRow=13\tCol=42\tCore_Row=13\tCore_Col=42: 13 genes x 42 conditions\n",
      "Block BC006\tS=442\tEnrichment:1.91\tRow=13\tCol=34\tCore_Row=13\tCore_Col=34: 13 genes x 34 conditions\n",
      "Block BC007\tS=221\tEnrichment:2.01\tRow=13\tCol=17\tCore_Row=13\tCore_Col=17: 13 genes x 17 conditions\n",
      "Block BC008\tS=288\tEnrichment:1.74\tRow=24\tCol=12\tCore_Row=24\tCore_Col=12: 24 genes x 12 conditions\n",
      "Block BC009\tS=540\tEnrichment:1.91\tRow=12\tCol=45\tCore_Row=12\tCore_Col=45: 12 genes x 45 conditions\n",
      "Block BC010\tS=288\tEnrichment:1.92\tRow=12\tCol=24\tCore_Row=12\tCore_Col=24: 12 genes x 24 conditions\n",
      "Block BC011\tS=240\tEnrichment:1.98\tRow=12\tCol=20\tCore_Row=12\tCore_Col=20: 12 genes x 20 conditions\n",
      "Block BC012\tS=516\tEnrichment:1.90\tRow=12\tCol=43\tCore_Row=12\tCore_Col=43: 12 genes x 43 conditions\n",
      "Block BC013\tS=252\tEnrichment:1.97\tRow=12\tCol=21\tCore_Row=12\tCore_Col=21: 12 genes x 21 conditions\n",
      "Block BC014\tS=528\tEnrichment:1.95\tRow=11\tCol=48\tCore_Row=11\tCore_Col=48: 11 genes x 48 conditions\n",
      "Block BC015\tS=495\tEnrichment:1.92\tRow=11\tCol=45\tCore_Row=11\tCore_Col=45: 11 genes x 45 conditions\n",
      "Block BC016\tS=308\tEnrichment:1.98\tRow=11\tCol=28\tCore_Row=11\tCore_Col=28: 11 genes x 28 conditions\n",
      "Block BC017\tS=350\tEnrichment:1.97\tRow=10\tCol=35\tCore_Row=10\tCore_Col=35: 10 genes x 35 conditions\n",
      "Block BC018\tS=210\tEnrichment:2.00\tRow=10\tCol=21\tCore_Row=10\tCore_Col=21: 10 genes x 21 conditions\n",
      "Block BC019\tS=486\tEnrichment:1.96\tRow=9\tCol=54\tCore_Row=9\tCore_Col=54: 9 genes x 54 conditions\n",
      "Block BC020\tS=171\tEnrichment:2.01\tRow=19\tCol=9\tCore_Row=19\tCore_Col=9: 19 genes x 9 conditions\n",
      "Block BC021\tS=176\tEnrichment:1.95\tRow=8\tCol=22\tCore_Row=8\tCore_Col=22: 8 genes x 22 conditions\n",
      "Block BC022\tS=336\tEnrichment:1.92\tRow=8\tCol=42\tCore_Row=8\tCore_Col=42: 8 genes x 42 conditions\n",
      "Block BC023\tS=88\tEnrichment:1.99\tRow=8\tCol=11\tCore_Row=8\tCore_Col=11: 8 genes x 11 conditions\n",
      "Block BC024\tS=224\tEnrichment:1.90\tRow=8\tCol=28\tCore_Row=8\tCore_Col=28: 8 genes x 28 conditions\n",
      "Block BC025\tS=175\tEnrichment:2.02\tRow=25\tCol=7\tCore_Row=20\tCore_Col=7: 25 genes x 7 conditions\n",
      "Block BC026\tS=133\tEnrichment:1.96\tRow=19\tCol=7\tCore_Row=19\tCore_Col=7: 19 genes x 7 conditions\n",
      "Block BC027\tS=98\tEnrichment:2.12\tRow=14\tCol=7\tCore_Row=12\tCore_Col=7: 14 genes x 7 conditions\n",
      "Block BC028\tS=175\tEnrichment:1.95\tRow=7\tCol=25\tCore_Row=7\tCore_Col=25: 7 genes x 25 conditions\n",
      "Block BC029\tS=91\tEnrichment:1.92\tRow=7\tCol=13\tCore_Row=7\tCore_Col=13: 7 genes x 13 conditions\n",
      "Block BC030\tS=270\tEnrichment:1.93\tRow=6\tCol=45\tCore_Row=6\tCore_Col=45: 6 genes x 45 conditions\n",
      "Block BC031\tS=252\tEnrichment:1.95\tRow=6\tCol=42\tCore_Row=6\tCore_Col=42: 6 genes x 42 conditions\n",
      "Block BC032\tS=132\tEnrichment:2.09\tRow=22\tCol=6\tCore_Row=22\tCore_Col=6: 22 genes x 6 conditions\n",
      "Block BC033\tS=174\tEnrichment:1.97\tRow=6\tCol=29\tCore_Row=6\tCore_Col=29: 6 genes x 29 conditions\n",
      "Block BC034\tS=144\tEnrichment:1.94\tRow=6\tCol=24\tCore_Row=6\tCore_Col=24: 6 genes x 24 conditions\n",
      "Block BC035\tS=230\tEnrichment:1.95\tRow=5\tCol=46\tCore_Row=5\tCore_Col=46: 5 genes x 46 conditions\n",
      "Block BC036\tS=260\tEnrichment:1.95\tRow=5\tCol=52\tCore_Row=5\tCore_Col=52: 5 genes x 52 conditions\n",
      "Block BC037\tS=215\tEnrichment:1.99\tRow=5\tCol=43\tCore_Row=5\tCore_Col=43: 5 genes x 43 conditions\n",
      "Block BC038\tS=100\tEnrichment:1.97\tRow=20\tCol=5\tCore_Row=20\tCore_Col=5: 20 genes x 5 conditions\n",
      "Block BC039\tS=152\tEnrichment:1.83\tRow=38\tCol=4\tCore_Row=38\tCore_Col=4: 38 genes x 4 conditions\n",
      "Block BC040\tS=92\tEnrichment:1.92\tRow=23\tCol=4\tCore_Row=21\tCore_Col=4: 23 genes x 4 conditions\n",
      "Block BC041\tS=108\tEnrichment:2.04\tRow=27\tCol=4\tCore_Row=25\tCore_Col=4: 27 genes x 4 conditions\n",
      "Block BC042\tS=87\tEnrichment:2.09\tRow=29\tCol=3\tCore_Row=29\tCore_Col=3: 29 genes x 3 conditions\n",
      "Block BC043\tS=105\tEnrichment:1.92\tRow=35\tCol=3\tCore_Row=34\tCore_Col=3: 35 genes x 3 conditions\n",
      "Block BC044\tS=117\tEnrichment:2.02\tRow=39\tCol=3\tCore_Row=27\tCore_Col=3: 39 genes x 3 conditions\n",
      "Block BC045\tS=279\tEnrichment:1.99\tRow=3\tCol=93\tCore_Row=3\tCore_Col=93: 3 genes x 93 conditions\n",
      "Block BC046\tS=201\tEnrichment:1.88\tRow=3\tCol=67\tCore_Row=3\tCore_Col=67: 3 genes x 67 conditions\n",
      "Block BC047\tS=144\tEnrichment:1.94\tRow=3\tCol=48\tCore_Row=3\tCore_Col=48: 3 genes x 48 conditions\n",
      "Block BC048\tS=138\tEnrichment:1.91\tRow=3\tCol=46\tCore_Row=3\tCore_Col=46: 3 genes x 46 conditions\n"
     ]
    }
   ],
   "source": [
    "# Print dimensions of each block\n",
    "for i, block in enumerate(blocks):\n",
    "    num_genes = len(genes[i])\n",
    "    num_conditions = len(conditions[i])\n",
    "    print(f\"Block {block}: {num_genes} genes x {num_conditions} conditions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   X..100130426  X..391343  ABCA17P.650655  ACTL9.284382  ACY3.91703  \\\n",
      "0     -0.481788   2.915469        0.488841     -0.399636   -0.288242   \n",
      "1     -0.481788  -0.407763       -1.891932     -0.399636    0.302379   \n",
      "2     -0.481788  -0.407763        0.628157     -0.399636    0.761346   \n",
      "3     -0.481788  -0.407763        0.700952     -0.399636    0.119867   \n",
      "4     -0.481788  -0.407763        0.493521     -0.399636    0.708159   \n",
      "\n",
      "   ADAMTS3.9508  AFAP1L1.134265   AGT.183  ALDH1A2.8854  ANGPTL3.27329  ...  \\\n",
      "0      0.344308        0.748717  0.747830      0.980916       0.231374  ...   \n",
      "1     -2.209718        0.018986 -1.759513      1.112930      -3.023792  ...   \n",
      "2      0.221723        0.040994  0.218279     -0.950297       0.386257  ...   \n",
      "3      0.382081        0.072354  0.214471     -0.950297       0.584656  ...   \n",
      "4      0.083007       -0.044447  0.166009     -0.950297       0.135580  ...   \n",
      "\n",
      "   hsa.mir.600  hsa.mir.656  hsa.mir.660  hsa.mir.665  hsa.mir.770  \\\n",
      "0    -0.481671    -0.449708    -1.103123     2.548649    -0.333965   \n",
      "1    -0.481671    -0.449708    -0.099830    -0.355721    -0.333965   \n",
      "2    -0.481671    -0.449708     0.660869    -0.355721    -0.333965   \n",
      "3     2.211404    -0.449708    -2.170264    -0.355721    -0.333965   \n",
      "4    -0.481671    -0.449708     0.889806    -0.355721    -0.333965   \n",
      "\n",
      "   hsa.mir.873  hsa.mir.9.1  hsa.mir.9.2  Survival  Death  \n",
      "0    -0.656039     0.161553     0.104138     150.0    0.0  \n",
      "1    -0.656039     0.110605     0.124147     212.0    1.0  \n",
      "2    -0.656039    -1.109444    -0.617416    1431.0    1.0  \n",
      "3     1.470026     0.383138     0.359110    2220.0    0.0  \n",
      "4    -0.656039    -0.201898    -0.151668     122.0    1.0  \n",
      "\n",
      "[5 rows x 1073 columns]\n",
      "[24, 69, 54, 11, 74, 90, 79, 129, 49, 84, 32, 104, 36, 116, 25, 119, 130, 99, 110, 51]\n",
      "[13, 129, 90, 69, 54, 49, 11, 74, 79, 84, 24, 32, 104, 116, 36]\n",
      "[10, 19, 138, 72, 109, 37, 95, 128, 153, 78, 2, 113, 117, 103, 27]\n",
      "[120, 145, 57, 49, 69, 54, 90, 129, 84, 119, 36, 24, 51, 104, 116]\n",
      "[27, 50, 19, 37, 72, 138, 109, 78, 95, 113, 2, 103, 117, 153]\n",
      "[19, 37, 72, 138, 109, 78, 95, 113, 128, 153, 2, 117, 103]\n",
      "[18, 129, 69, 90, 49, 54, 11, 79, 84, 74, 119, 99, 116]\n",
      "[67, 112, 132, 37, 19, 109, 78, 138, 72, 95, 23, 62, 113]\n",
      "[39, 123, 28, 47, 92, 102, 107, 105, 26, 68, 111, 58, 0, 7, 55, 93, 44, 41, 22, 133, 45, 137, 155, 82]\n",
      "[69, 129, 54, 49, 90, 11, 79, 74, 84, 104, 116, 99]\n",
      "[7, 65, 21, 88, 55, 121, 60, 91, 64, 146, 44, 93]\n",
      "[112, 132, 78, 19, 109, 37, 138, 72, 95, 27, 103, 117]\n",
      "[48, 69, 11, 90, 129, 54, 74, 79, 84, 49, 104, 116]\n",
      "[113, 157, 95, 138, 19, 109, 37, 72, 128, 2, 117, 27]\n",
      "[37, 112, 19, 78, 109, 72, 138, 95, 128, 153, 113]\n",
      "[23, 37, 19, 109, 72, 78, 138, 95, 113, 62, 128]\n",
      "[95, 126, 113, 138, 19, 109, 37, 72, 78, 23, 153]\n",
      "[14, 95, 19, 109, 78, 37, 138, 72, 23, 62]\n",
      "[19, 75, 95, 138, 72, 78, 37, 109, 23, 62]\n",
      "[20, 112, 37, 78, 109, 19, 72, 138, 95]\n",
      "[47, 142, 28, 123, 92, 105, 39, 68, 26, 102, 58, 0, 107, 108, 111, 88, 7, 21, 93]\n",
      "[62, 94, 152, 3, 2, 113, 73, 128]\n",
      "[69, 120, 129, 49, 119, 54, 90, 84]\n",
      "[127, 154, 131, 143, 34, 3, 73, 53]\n",
      "[69, 118, 79, 11, 49, 90, 54, 84]\n",
      "[19, 156, 78, 72, 109, 37, 138, 95, 2, 128, 153, 103, 62, 112, 127, 73, 122, 46, 3, 94, 64, 84, 88, 91, 121]\n",
      "[131, 149, 143, 59, 12, 73, 97, 35, 101, 63, 16, 87, 80, 53, 127, 100, 158, 17, 18]\n",
      "[69, 115, 49, 129, 54, 79, 8, 11, 90, 18, 57, 84, 27, 95]\n",
      "[96, 127, 131, 2, 46, 153, 103]\n",
      "[46, 147, 128, 117, 50, 2, 62]\n",
      "[1, 32, 69, 11, 74, 24]\n",
      "[131, 150, 127, 153, 128, 2]\n",
      "[4, 45, 82, 7, 133, 155, 71, 21, 39, 94, 111, 58, 14, 62, 152, 3, 27, 50, 63, 65, 68, 107]\n",
      "[6, 63, 128, 153, 2, 50]\n",
      "[134, 136, 18, 8, 29, 66]\n",
      "[15, 32, 141, 104, 144]\n",
      "[18, 29, 134, 8, 49]\n",
      "[30, 113, 95, 138, 19]\n",
      "[3, 33, 133, 27, 28, 92, 47, 0, 58, 123, 68, 26, 102, 39, 105, 82, 107, 111, 45, 7]\n",
      "[7, 21, 55, 93, 44, 102, 123, 28, 107, 92, 26, 47, 105, 39, 68, 58, 111, 121, 64, 124, 41, 81, 60, 146, 22, 71, 45, 137, 82, 83, 91, 88, 114, 140, 27, 98, 66, 155]\n",
      "[56, 61, 43, 97, 66, 86, 125, 34, 100, 127, 131, 143, 31, 52, 12, 57, 59, 70, 77, 80, 101, 158, 82]\n",
      "[42, 127, 131, 76, 53, 12, 73, 97, 59, 101, 87, 80, 143, 100, 148, 17, 18, 89, 31, 5, 9, 52, 77, 85, 153, 19, 72]\n",
      "[39, 108, 68, 58, 47, 28, 123, 0, 92, 105, 26, 102, 107, 111, 93, 7, 44, 45, 133, 137, 94, 82, 155, 55, 41, 22, 27, 140, 3]\n",
      "[76, 131, 127, 34, 63, 153, 2, 62, 103, 46, 122, 59, 101, 73, 35, 53, 12, 43, 97, 148, 8, 17, 18, 29, 49, 69, 70, 79, 80, 87, 100, 134, 151, 158, 37]\n",
      "[16, 63, 153, 128, 2, 46, 122, 103, 10, 56, 19, 37, 95, 109, 113, 138, 72, 20, 112, 62, 78, 117, 125, 94, 27, 38, 143, 7, 11, 21, 54, 64, 69, 79, 84, 88, 90, 102, 129]\n",
      "[40, 74, 11]\n",
      "[49, 106, 129]\n",
      "[5, 139, 53]\n",
      "[135, 143, 131]\n"
     ]
    }
   ],
   "source": [
    "multi_omic_df = pd.read_csv('filtered_features_anova.tsv', sep=\"\\t\", on_bad_lines=\"skip\")\n",
    "# Extract the first row and assign it as column names\n",
    "#multi_omic_df.columns = multi_omic_df.iloc[0]\n",
    "#multi_omic_df = multi_omic_df[1:]\n",
    "# Reset index\n",
    "multi_omic_df.reset_index(drop=True, inplace=True)\n",
    "print(multi_omic_df.head())\n",
    "\n",
    "# Clean column names in the multi-omic DataFrame\n",
    "multi_omic_df.columns = [col.strip() for col in multi_omic_df.columns]\n",
    "\n",
    "# Clean gene and condition names in the blocks\n",
    "genes = [[gene.strip() for gene in sublist] for sublist in genes]\n",
    "conditions = [[cond.strip() for cond in sublist] for sublist in conditions]\n",
    "\n",
    "# Initialize a list to store the DataFrames for each block\n",
    "block_dataframes = []\n",
    "\n",
    "# Iterate through each block\n",
    "for i in range(len(blocks)):\n",
    "    # Extract genes and conditions for the current block\n",
    "    current_genes = [int(gene) for gene in genes[i] if int(gene) in multi_omic_df.index]\n",
    "    current_conditions = [cond for cond in conditions[i] if cond in multi_omic_df.columns]\n",
    "    print(current_genes)\n",
    "\n",
    "    # Include the 'Death' column in the conditions\n",
    "    if 'Death' in multi_omic_df.columns and 'Death' not in current_conditions:\n",
    "        current_conditions.append('Death')\n",
    "\n",
    "    # Skip the block if no valid genes or conditions are found\n",
    "    if not current_genes or not current_conditions:\n",
    "        print(f\"Skipping block {i} due to missing genes or conditions.\")\n",
    "        continue\n",
    "\n",
    "    filtered_column = multi_omic_df[current_conditions]\n",
    "    filtered_df = filtered_column.loc[current_genes]\n",
    "\n",
    "    # Append the filtered DataFrame to the list\n",
    "    block_dataframes.append(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0:\n",
      "    cg00715696  cg01515515  cg02137956  cg05206633  cg05445326  cg07793808  \\\n",
      "24    0.185876   -1.269568   -1.016409   -1.069062    0.129975    0.736492   \n",
      "69    0.308138   -1.380611   -1.093566   -0.911416    0.678798   -0.989227   \n",
      "54    0.338137   -1.291941   -1.081857   -1.022546   -0.493158   -0.571720   \n",
      "11   -0.860111   -1.305750   -1.221979    0.292566    0.753872   -0.636795   \n",
      "74   -0.552951   -1.289200   -1.285550   -0.326682    0.669231   -0.331147   \n",
      "\n",
      "    cg08173263  cg09156207  cg10741025  cg10940462  ...  cg19047660  \\\n",
      "24   -0.709517   -1.809951    0.340001    1.042801  ...   -0.731941   \n",
      "69   -0.767837   -2.106895   -1.161439   -0.924919  ...   -0.762747   \n",
      "54   -0.803260   -1.967681   -1.189924   -0.874262  ...   -0.727016   \n",
      "11   -0.833295    0.294038   -1.265214   -0.914620  ...   -0.789748   \n",
      "74   -0.738020    0.769057   -0.721897   -0.772268  ...   -0.769493   \n",
      "\n",
      "    cg20793665  cg22222281  cg23530553  cg23531049  cg25496181  cg25801976  \\\n",
      "24   -0.353933   -0.114193   -1.109468   -0.041340   -0.548889   -0.717936   \n",
      "69   -2.000094   -1.321523   -1.279894   -0.002431   -0.559803   -0.651613   \n",
      "54   -1.697877   -1.236182   -1.306453   -1.387963   -0.552015   -0.998765   \n",
      "11   -0.961115   -1.427995   -1.386944   -1.583010   -0.542614   -0.765486   \n",
      "74   -0.965739   -0.980075   -1.356512   -1.286006   -0.454578   -0.392778   \n",
      "\n",
      "    cg26376241  hsa.mir.1248  Death  \n",
      "24   -0.746749     -0.232714    1.0  \n",
      "69   -0.783611      0.254422    0.0  \n",
      "54   -0.647694     -0.441840    1.0  \n",
      "11   -0.743258     -0.306302    1.0  \n",
      "74   -0.760182     -4.130658    1.0  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "\n",
      "\n",
      "Block 1:\n",
      "     CT45A3.441519  cg00715696  cg00997251  cg01515515  cg01538731  \\\n",
      "13        1.963018   -0.606910   -1.643531   -1.357797    0.498934   \n",
      "129       1.976984   -0.783281   -1.600753   -0.894936    0.515532   \n",
      "90        1.860919   -0.895759   -1.620188   -1.198005    0.107246   \n",
      "69        1.976512    0.308138   -0.922070   -1.380611   -0.898736   \n",
      "54       -0.544615    0.338137   -1.746616   -1.291941   -0.716921   \n",
      "\n",
      "     cg02137956  cg03982355  cg05206633  cg05445326  cg06474225  ...  \\\n",
      "13    -0.987182   -0.776919   -1.016723    0.237874   -0.115293  ...   \n",
      "129   -1.117600   -0.850268   -2.035748    0.538980   -0.257536  ...   \n",
      "90    -1.095093   -0.800948   -1.089864    0.293672   -0.225593  ...   \n",
      "69    -1.093566   -0.785147   -0.911416    0.678798    1.197983  ...   \n",
      "54    -1.081857   -0.819430   -1.022546   -0.493158   -0.647588  ...   \n",
      "\n",
      "     cg20513976  cg21387009  cg22222281  cg23531049  cg25496181  cg25801976  \\\n",
      "13    -0.836379   -0.951609   -0.322471   -0.379691   -0.533363   -0.367134   \n",
      "129   -0.445350   -1.067494   -1.412466   -1.121652   -0.539442   -1.162469   \n",
      "90    -0.903967   -1.099237   -1.094902   -0.929490    2.005081   -0.489358   \n",
      "69    -0.904293   -0.887919   -1.321523   -0.002431   -0.559803   -0.651613   \n",
      "54    -0.900041   -1.024395   -1.236182   -1.387963   -0.552015   -0.998765   \n",
      "\n",
      "     cg26147845  cg26376241  hsa.mir.1248  Death  \n",
      "13    -0.969429   -0.720898      0.640073    1.0  \n",
      "129   -0.981728   -0.753830      0.355757    0.0  \n",
      "90    -0.951375   -0.713237     -4.130658    0.0  \n",
      "69    -0.954546   -0.783611      0.254422    0.0  \n",
      "54    -0.916243   -0.647694     -0.441840    1.0  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "\n",
      "\n",
      "Block 2:\n",
      "     cg00532474  cg01252023  cg02239258  cg03956042  cg03972560  cg04857395  \\\n",
      "10     0.764496    0.910100    0.452222    1.651697    0.300107    0.466385   \n",
      "19     1.603039    1.107286    0.501488    1.815947    1.895967    0.723857   \n",
      "138    1.270045    1.018798    0.277706    1.397628   -0.823329    0.678098   \n",
      "72     1.602759    1.080239    1.037153    0.245865    0.387697    0.714680   \n",
      "109    1.442917    1.068770    0.405859    0.672184    1.627476    0.714562   \n",
      "\n",
      "     cg07195011  cg08698943  cg09232555  cg09462281  ...  cg15417249  \\\n",
      "10    -0.510556    1.596206   -0.345926    0.763674  ...    1.312289   \n",
      "19     0.289035   -0.621943    1.768066    0.895609  ...    1.758569   \n",
      "138   -0.617231   -0.582110    1.937537    0.914651  ...    1.616015   \n",
      "72     0.532748   -0.821063    0.737658    0.900410  ...    1.606167   \n",
      "109   -0.263087   -0.497279    1.132758    0.927577  ...    1.704019   \n",
      "\n",
      "     cg18384588  cg19537719  cg22731271  cg23171972  cg23528247  cg23690893  \\\n",
      "10     1.107083   -1.121042   -0.518986    0.803942    0.056440    0.486400   \n",
      "19     1.069137   -0.626250   -0.674525   -0.263199    0.412795    1.398669   \n",
      "138   -0.893566   -1.583171   -0.655238    0.799461    0.126545    1.331006   \n",
      "72    -0.070237    1.068187   -0.670746    0.862851    0.686058    1.370316   \n",
      "109   -0.748952   -0.191624   -0.687868    0.584705   -1.176676    1.344417   \n",
      "\n",
      "     cg26235215  cg26607828  Death  \n",
      "10    -0.665142   -0.668360    1.0  \n",
      "19    -0.558523    2.129514    0.0  \n",
      "138   -0.544747    1.452583    1.0  \n",
      "72    -0.414957    1.982518    0.0  \n",
      "109   -0.321577    1.550916    1.0  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "\n",
      "Block 3:\n",
      "     CT45A3.441519  GPR142.350383  cg01515515  cg02137956  cg10741025  \\\n",
      "120       2.087717      -0.963051   -1.325512   -0.437580   -0.991510   \n",
      "145       2.294092       0.974708   -1.142560    0.297439   -0.999609   \n",
      "57        1.606410       1.297366    0.368987    1.430862   -1.234295   \n",
      "49        1.917151       0.860364   -1.331073   -0.101646   -1.107749   \n",
      "69        1.976512      -0.963051   -1.380611   -1.093566   -1.161439   \n",
      "\n",
      "     cg10940462  cg13481969  cg13979887  cg14473743  cg17413460  cg20513976  \\\n",
      "120   -0.650151   -0.468628    0.743231   -0.811363    0.629023    1.755670   \n",
      "145   -0.889943   -0.938176    0.638055   -0.906848   -0.898767    1.531320   \n",
      "57    -0.937806   -0.533669    0.796811   -0.901429   -2.016418   -0.823618   \n",
      "49    -0.859825   -0.955907   -1.483771   -0.974548   -2.110953   -0.670490   \n",
      "69    -0.924919   -0.953766   -1.399398   -0.977585   -2.206631   -0.904293   \n",
      "\n",
      "     cg20793665  cg23530553  cg24105081  cg25496181  cg25801976  cg26376241  \\\n",
      "120    1.125126    0.163715   -1.352509   -0.543202   -1.110142   -0.709385   \n",
      "145    1.112374    0.490949    0.237225   -0.557123   -0.912131   -0.729177   \n",
      "57    -1.817713   -0.755387   -1.189882   -0.550945   -0.929014   -0.643342   \n",
      "49     0.463425   -1.370930   -1.097691   -0.533896   -0.828997   -0.764982   \n",
      "69    -2.000094   -1.279894   -1.315774   -0.559803   -0.651613   -0.783611   \n",
      "\n",
      "     rs1510189  Death  \n",
      "120   0.018035    0.0  \n",
      "145   0.041489    1.0  \n",
      "57    0.029396    1.0  \n",
      "49    1.504696    1.0  \n",
      "69    0.011116    0.0  \n",
      "\n",
      "\n",
      "Block 4:\n",
      "    cg00532474  cg01208318  cg01252023  cg02239258  cg03672272  cg03956042  \\\n",
      "27    1.056298    1.292841    1.113785   -0.683530    1.890130    1.914871   \n",
      "50    1.342950    1.019157    1.077389   -0.273302    1.039745    1.722455   \n",
      "19    1.603039    1.260141    1.107286    0.501488    0.671111    1.815947   \n",
      "37    0.912602    1.199791    1.125467    1.529096   -0.944901    0.610499   \n",
      "72    1.602759    1.276419    1.080239    1.037153   -0.935241    0.245865   \n",
      "\n",
      "    cg03972560  cg04857395  cg07195011  cg08698943  ...  cg18384588  \\\n",
      "27   -0.938951   -1.101241    0.757180    1.979858  ...    1.096650   \n",
      "50   -1.119150   -0.659791   -0.587460    1.893503  ...    1.009709   \n",
      "19    1.895967    0.723857    0.289035   -0.621943  ...    1.069137   \n",
      "37    0.505295    0.733058   -0.464110   -0.285109  ...    0.746581   \n",
      "72    0.387697    0.714680    0.532748   -0.821063  ...   -0.070237   \n",
      "\n",
      "    cg19537719  cg19893929  cg22731271  cg23528247  cg23690893  cg24675150  \\\n",
      "27    0.828597   -0.329406    0.180790    1.269162    0.085560    0.905310   \n",
      "50   -1.021945   -0.852121    1.951323    0.659186    1.017186    0.947808   \n",
      "19   -0.626250    1.127083   -0.674525    0.412795    1.398669   -1.039419   \n",
      "37    1.032740    1.136126   -0.704194   -0.413578    1.415139   -1.045516   \n",
      "72    1.068187    1.073591   -0.670746    0.686058    1.370316   -1.058976   \n",
      "\n",
      "    cg26235215  cg26607828  Death  \n",
      "27   -0.675873    2.142302    0.0  \n",
      "50   -0.715526   -0.724726    1.0  \n",
      "19   -0.558523    2.129514    0.0  \n",
      "37   -0.696888    1.795964    0.0  \n",
      "72   -0.414957    1.982518    0.0  \n",
      "\n",
      "[5 rows x 32 columns]\n",
      "\n",
      "\n",
      "Block 5:\n",
      "     cg00532474  cg01252023  cg02239258  cg03548415  cg03956042  cg03972560  \\\n",
      "19     1.603039    1.107286    0.501488    1.532449    1.815947    1.895967   \n",
      "37     0.912602    1.125467    1.529096    1.456615    0.610499    0.505295   \n",
      "72     1.602759    1.080239    1.037153    1.282064    0.245865    0.387697   \n",
      "138    1.270045    1.018798    0.277706    1.368646    1.397628   -0.823329   \n",
      "109    1.442917    1.068770    0.405859    0.874082    0.672184    1.627476   \n",
      "\n",
      "     cg04857395  cg05301188  cg07195011  cg08347500  ...  cg22731271  \\\n",
      "19     0.723857    1.663560    0.289035    2.521467  ...   -0.674525   \n",
      "37     0.733058   -0.191998   -0.464110    0.175022  ...   -0.704194   \n",
      "72     0.714680    1.435758    0.532748    1.802788  ...   -0.670746   \n",
      "138    0.678098    0.819696   -0.617231    1.652588  ...   -0.655238   \n",
      "109    0.714562    1.116127   -0.263087    1.233487  ...   -0.687868   \n",
      "\n",
      "     cg23171972  cg23528247  cg23690893  cg23732024  cg24675150  cg25921609  \\\n",
      "19    -0.263199    0.412795    1.398669    1.172845   -1.039419    0.887492   \n",
      "37     0.768709   -0.413578    1.415139    0.019012   -1.045516    0.904853   \n",
      "72     0.862851    0.686058    1.370316    1.161663   -1.058976    0.886347   \n",
      "138    0.799461    0.126545    1.331006    1.157421   -0.258644    0.862306   \n",
      "109    0.584705   -1.176676    1.344417    1.185507   -0.932201    0.875005   \n",
      "\n",
      "     cg26235215  cg26607828  Death  \n",
      "19    -0.558523    2.129514    0.0  \n",
      "37    -0.696888    1.795964    0.0  \n",
      "72    -0.414957    1.982518    0.0  \n",
      "138   -0.544747    1.452583    1.0  \n",
      "109   -0.321577    1.550916    1.0  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "\n",
      "\n",
      "Block 6:\n",
      "     CT45A3.441519  cg01089498  cg01515515  cg02137956  cg05206633  \\\n",
      "18        1.568008   -0.375951   -1.338348    0.301799   -1.148695   \n",
      "129       1.976984   -0.772813   -0.894936   -1.117600   -2.035748   \n",
      "69        1.976512   -0.755035   -1.380611   -1.093566   -0.911416   \n",
      "90        1.860919   -0.754776   -1.198005   -1.095093   -1.089864   \n",
      "49        1.917151   -0.786061   -1.331073   -0.101646   -1.224662   \n",
      "\n",
      "     cg05445326  cg05915866  cg06443533  cg07110356  cg07793808  ...  \\\n",
      "18    -1.306596   -0.962004   -0.812813   -0.684902   -1.270312  ...   \n",
      "129    0.538980   -1.355336    0.128892   -1.424043   -0.387339  ...   \n",
      "69     0.678798   -1.355208   -0.900466    1.163655   -0.989227  ...   \n",
      "90     0.293672    0.090833   -0.881351    0.372557   -1.120003  ...   \n",
      "49    -0.397598   -1.279027   -0.891557   -1.175289   -1.267547  ...   \n",
      "\n",
      "     cg19047660  cg20513976  cg20793665  cg22222281  cg23530553  cg23531049  \\\n",
      "18    -0.681786    1.266461   -0.514904   -1.152687   -1.051004   -0.616017   \n",
      "129   -0.802339   -0.445350   -0.468374   -1.412466   -1.540623   -1.121652   \n",
      "69    -0.762747   -0.904293   -2.000094   -1.321523   -1.279894   -0.002431   \n",
      "90    -0.767346   -0.903967   -0.438118   -1.094902   -1.346822   -0.929490   \n",
      "49    -0.802550   -0.670490    0.463425   -1.134652   -1.370930   -1.166170   \n",
      "\n",
      "     cg25801976  cg26376241  hsa.mir.1248  Death  \n",
      "18    -0.949175   -0.682423      0.548967    1.0  \n",
      "129   -1.162469   -0.753830      0.355757    0.0  \n",
      "69    -0.651613   -0.783611      0.254422    0.0  \n",
      "90    -0.489358   -0.713237     -4.130658    0.0  \n",
      "49    -0.828997   -0.764982      0.111155    1.0  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "\n",
      "\n",
      "Block 7:\n",
      "     cg03967627  cg04655481  cg07816074  cg08943714  cg09232555  cg09462281  \\\n",
      "67     0.866599   -1.122041    0.778972   -1.273029    1.851096    0.830366   \n",
      "112    1.685146   -1.178068    0.889386   -1.226931    0.408345    0.833136   \n",
      "132   -0.340519   -1.144814    0.772083   -0.748823    1.054588   -0.349550   \n",
      "37     1.708126   -1.137842    0.856177    0.201852    1.944867    0.949101   \n",
      "19     1.685742   -1.025275    0.856816    1.431151    1.768066    0.895609   \n",
      "\n",
      "     cg11227278  cg12057563  cg13005202  cg13073699  cg14381350  cg15219228  \\\n",
      "67    -0.706290    0.780213   -0.963278    0.589764    0.570834    0.753401   \n",
      "112    0.559172    0.781721   -0.991205   -0.767582    1.025896    0.718432   \n",
      "132   -0.168815    0.874857   -0.898970   -0.814494    1.019088    1.168746   \n",
      "37     1.329896    0.884022   -0.672268    1.838718    1.047843    1.078011   \n",
      "19     1.174246    0.838465   -0.278728    1.891161    0.929863    1.065483   \n",
      "\n",
      "     cg16409562  cg18179039  cg23732024  cg26607828  cg27214856  Death  \n",
      "67     0.767138   -1.107561    0.465922   -0.636471   -1.648608    1.0  \n",
      "112    1.290642    1.229261    0.122810   -0.291138   -1.720363    1.0  \n",
      "132    1.342400    1.078515    1.146758   -0.409101   -1.664800    0.0  \n",
      "37     1.313618    1.810936    0.019012    1.795964    0.837543    0.0  \n",
      "19     1.318578    1.689197    1.172845    2.129514    0.790021    0.0  \n",
      "\n",
      "\n",
      "Block 8:\n",
      "     cg01538731  cg05206633  cg06363129  cg09122035  cg12542656  cg13657981  \\\n",
      "39    -1.290545   -1.509505    0.413915    0.839988    1.714201    0.731855   \n",
      "123   -1.498291   -1.577201    0.447599    0.969229    1.694667    1.051044   \n",
      "28    -1.444798   -1.697676    0.412377    1.046294    1.905717    1.048465   \n",
      "47    -0.809142   -1.516377    0.707587    0.862551    1.647668    1.002183   \n",
      "92    -1.527020   -1.760653    0.705239    0.902911    1.895798    0.752947   \n",
      "\n",
      "     cg14603098  cg14661139  cg14967804  cg23884076  cg25456368  cg25953130  \\\n",
      "39     1.387089    0.016510   -1.300672    1.749495    1.770802    0.954064   \n",
      "123    1.872919   -1.229461    1.021152    1.847552    0.641682    1.085105   \n",
      "28     0.405093    0.175194    0.796560    1.731945    0.110892    0.732905   \n",
      "47    -0.557342    0.099819   -1.275128    1.829997    0.532765    1.003306   \n",
      "92    -0.713964    0.450473   -0.875956    1.930032    1.815069    1.029477   \n",
      "\n",
      "     Death  \n",
      "39     1.0  \n",
      "123    1.0  \n",
      "28     0.0  \n",
      "47     1.0  \n",
      "92     0.0  \n",
      "\n",
      "\n",
      "Block 9:\n",
      "     CT45A3.441519  cg00715696  cg01089498  cg01515515  cg02137956  \\\n",
      "69        1.976512    0.308138   -0.755035   -1.380611   -1.093566   \n",
      "129       1.976984   -0.783281   -0.772813   -0.894936   -1.117600   \n",
      "54       -0.544615    0.338137   -0.763928   -1.291941   -1.081857   \n",
      "49        1.917151   -0.669771   -0.786061   -1.331073   -0.101646   \n",
      "90        1.860919   -0.895759   -0.754776   -1.198005   -1.095093   \n",
      "\n",
      "     cg03982355  cg05206633  cg05445326  cg05915866  cg06443533  ...  \\\n",
      "69    -0.785147   -0.911416    0.678798   -1.355208   -0.900466  ...   \n",
      "129   -0.850268   -2.035748    0.538980   -1.355336    0.128892  ...   \n",
      "54    -0.819430   -1.022546   -0.493158   -0.015540   -0.674807  ...   \n",
      "49    -0.735496   -1.224662   -0.397598   -1.279027   -0.891557  ...   \n",
      "90    -0.800948   -1.089864    0.293672    0.090833   -0.881351  ...   \n",
      "\n",
      "     cg23415434  cg23530553  cg23531049  cg24123198  cg25496181  cg25801976  \\\n",
      "69    -1.608803   -1.279894   -0.002431   -1.726423   -0.559803   -0.651613   \n",
      "129   -0.345959   -1.540623   -1.121652   -1.752882   -0.539442   -1.162469   \n",
      "54    -0.366678   -1.306453   -1.387963   -0.901999   -0.552015   -0.998765   \n",
      "49     1.141801   -1.370930   -1.166170   -0.842274   -0.533896   -0.828997   \n",
      "90     0.293878   -1.346822   -0.929490   -1.685938    2.005081   -0.489358   \n",
      "\n",
      "     cg26147845  cg26376241  hsa.mir.1248  Death  \n",
      "69    -0.954546   -0.783611      0.254422    0.0  \n",
      "129   -0.981728   -0.753830      0.355757    0.0  \n",
      "54    -0.916243   -0.647694     -0.441840    1.0  \n",
      "49    -0.833600   -0.764982      0.111155    1.0  \n",
      "90    -0.951375   -0.713237     -4.130658    0.0  \n",
      "\n",
      "[5 rows x 46 columns]\n",
      "\n",
      "\n",
      "Block 10:\n",
      "    cg01542019  cg02340818  cg02367655  cg02680086  cg03309393  cg04656042  \\\n",
      "7    -0.830692   -1.352866   -0.297421   -1.567430    1.651409    0.833942   \n",
      "65    0.408952   -1.263807    2.007093   -0.211230    1.609052    1.834610   \n",
      "21   -1.843936   -1.054295    1.919747   -1.634602    1.578931    0.670842   \n",
      "88   -2.038053   -1.371329    1.774480   -0.345533    1.641328    1.331981   \n",
      "55    0.015244   -1.107822   -0.332200    0.055200    1.664718   -1.236229   \n",
      "\n",
      "    cg04858586  cg05856321  cg08447324  cg08525314  ...  cg15070894  \\\n",
      "7     1.387234   -0.460399    2.123409   -1.816783  ...   -1.520549   \n",
      "65    1.313890    2.067166    2.378933   -1.834048  ...    1.369380   \n",
      "21   -0.338902    1.781277    2.206404   -1.895287  ...   -0.038928   \n",
      "88   -1.373679    0.691556    2.285612   -1.930854  ...   -0.811898   \n",
      "55    1.293305   -0.437209    1.662394   -1.722393  ...    0.445860   \n",
      "\n",
      "    cg17534999  cg18179039  cg19966212  cg20359042  cg25145765  cg25953130  \\\n",
      "7     0.861003    1.800966   -0.551256    0.764911    2.091268    0.918756   \n",
      "65    0.813548    0.841631   -0.526909   -1.119807    1.217717    0.806469   \n",
      "21    0.784724    0.905774   -0.511008   -1.768385    2.052258    0.890395   \n",
      "88    0.570224   -1.014212   -0.521187   -2.092297    1.385538    0.679885   \n",
      "55   -1.004318    1.779669   -0.475572    0.784533    1.923911    0.975617   \n",
      "\n",
      "    cg26607828  cg27072996  Death  \n",
      "7     2.006659   -0.714067    1.0  \n",
      "65    2.149049    1.279928    0.0  \n",
      "21    1.604155   -0.728629    0.0  \n",
      "88    2.013861   -0.734689    1.0  \n",
      "55    0.977805   -0.689164    0.0  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "\n",
      "\n",
      "Block 11:\n",
      "     cg01208318  cg02367655  cg03967627  cg08943714  cg09232555  cg09462281  \\\n",
      "112    1.303747   -0.346165    1.685146   -1.226931    0.408345    0.833136   \n",
      "132    1.165878    0.407334   -0.340519   -0.748823    1.054588   -0.349550   \n",
      "78     1.267097    1.142559    1.705344    1.292595    1.772339    0.938887   \n",
      "19     1.260141    1.868286    1.685742    1.431151    1.768066    0.895609   \n",
      "109    1.208797    1.345133    1.582459    0.160713    1.132758    0.927577   \n",
      "\n",
      "     cg10473367  cg12057563  cg12116137  cg12448285  ...  cg14381350  \\\n",
      "112    0.759347    0.781721    1.195621   -0.331594  ...    1.025896   \n",
      "132   -0.114923    0.874857    1.194313   -1.571539  ...    1.019088   \n",
      "78    -0.128901    0.870085    1.192303    1.012067  ...    1.042742   \n",
      "19     1.142704    0.838465    1.197817    0.977313  ...    0.929863   \n",
      "109    1.216848    0.849144    1.198999    1.016863  ...    0.821067   \n",
      "\n",
      "     cg14499274  cg14531862  cg15417249  cg15419294  cg18384588  cg21937377  \\\n",
      "112    1.182344   -0.547899    1.612376    1.185957   -1.264167    0.886484   \n",
      "132    0.932385    0.530644    1.611935    1.143460   -1.323724    0.882696   \n",
      "78     1.194935   -1.001803    1.684294   -0.161292   -1.061905    0.882129   \n",
      "19     1.215179    1.892420    1.758569    1.211215    1.069137    0.895641   \n",
      "109    1.173339    0.418223    1.704019   -0.216995   -0.748952    0.857564   \n",
      "\n",
      "     cg24675150  cg26607828  Death  \n",
      "112   -0.818478   -0.291138    1.0  \n",
      "132    0.830980   -0.409101    0.0  \n",
      "78     1.789698    1.914871    1.0  \n",
      "19    -1.039419    2.129514    0.0  \n",
      "109   -0.932201    1.550916    1.0  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "\n",
      "Block 12:\n",
      "     CT45A3.441519  cg00715696  cg00997251  cg01089498  cg01515515  \\\n",
      "48        2.133940   -0.830225   -1.474042   -0.553503    0.820864   \n",
      "69        1.976512    0.308138   -0.922070   -0.755035   -1.380611   \n",
      "11        1.901415   -0.860111   -1.301010   -0.780971   -1.305750   \n",
      "90        1.860919   -0.895759   -1.620188   -0.754776   -1.198005   \n",
      "129       1.976984   -0.783281   -1.600753   -0.772813   -0.894936   \n",
      "\n",
      "     cg01538731  cg02137956  cg03982355  cg05445326  cg05915866  ...  \\\n",
      "48     0.979852   -1.185676   -0.735342    0.771526   -1.211100  ...   \n",
      "69    -0.898736   -1.093566   -0.785147    0.678798   -1.355208  ...   \n",
      "11     1.139415   -1.221979   -0.818434    0.753872   -0.840401  ...   \n",
      "90     0.107246   -1.095093   -0.800948    0.293672    0.090833  ...   \n",
      "129    0.515532   -1.117600   -0.850268    0.538980   -1.355336  ...   \n",
      "\n",
      "     cg22222281  cg23530553  cg23531049  cg24123198  cg25496181  cg25801976  \\\n",
      "48     0.372036   -1.303696   -0.869620    0.325902   -0.524939    1.315829   \n",
      "69    -1.321523   -1.279894   -0.002431   -1.726423   -0.559803   -0.651613   \n",
      "11    -1.427995   -1.386944   -1.583010   -0.936844   -0.542614   -0.765486   \n",
      "90    -1.094902   -1.346822   -0.929490   -1.685938    2.005081   -0.489358   \n",
      "129   -1.412466   -1.540623   -1.121652   -1.752882   -0.539442   -1.162469   \n",
      "\n",
      "     cg26147845  cg26376241  hsa.mir.1248  Death  \n",
      "48    -0.930340   -0.631794      0.093687    0.0  \n",
      "69    -0.954546   -0.783611      0.254422    0.0  \n",
      "11    -0.942988   -0.743258     -0.306302    1.0  \n",
      "90    -0.951375   -0.713237     -4.130658    0.0  \n",
      "129   -0.981728   -0.753830      0.355757    0.0  \n",
      "\n",
      "[5 rows x 44 columns]\n",
      "\n",
      "\n",
      "Block 13:\n",
      "     cg00532474  cg02891728  cg03173502  cg03604774  cg03972560  cg04857395  \\\n",
      "113    1.582973    1.196086    0.841022    0.710785   -0.676804    0.737095   \n",
      "157    0.313905    0.619498    0.874674    0.796394    0.312613    0.787080   \n",
      "95     1.564413    1.187237    0.788027    0.665635   -0.647685    0.704601   \n",
      "138    1.270045    1.190886   -1.517082    0.784537   -0.823329    0.678098   \n",
      "19     1.603039    1.191803   -1.951312    0.792023    1.895967    0.723857   \n",
      "\n",
      "     cg06283493  cg08447324  cg09232555  cg09975576  ...  cg14371731  \\\n",
      "113   -1.175791    0.194386   -0.716534    0.901719  ...    1.397014   \n",
      "157   -1.071851   -0.605631    1.933205   -0.468247  ...   -0.734216   \n",
      "95     0.641275   -0.614030    1.803697    0.797403  ...    1.449624   \n",
      "138    0.649908   -0.604583    1.937537    1.002054  ...    1.460344   \n",
      "19     0.205547   -0.619966    1.768066    0.959270  ...    1.735282   \n",
      "\n",
      "     cg15090899  cg15417249  cg19893929  cg22731271  cg23171972  cg23690893  \\\n",
      "113    0.708088   -1.251485    1.076908    1.238204    0.786234    1.413717   \n",
      "157    0.727165   -0.786723    1.101504   -0.691361   -1.321722    1.327631   \n",
      "95     0.690099    1.226903    1.089562    1.552253    0.858175    1.401018   \n",
      "138    0.714860    1.616015    1.032123   -0.655238    0.799461    1.331006   \n",
      "19     0.718532    1.758569    1.127083   -0.674525   -0.263199    1.398669   \n",
      "\n",
      "     cg25145765  cg26607828  Death  \n",
      "113    0.238731    1.845494    1.0  \n",
      "157   -0.827117   -0.747062    0.0  \n",
      "95    -0.207781    1.677690    1.0  \n",
      "138   -0.641508    1.452583    1.0  \n",
      "19    -0.600084    2.129514    0.0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "\n",
      "Block 14:\n",
      "     cg00532474  cg00987534  cg01724917  cg02239258  cg03972560  cg03982355  \\\n",
      "37     0.912602   -0.933457    0.805568    1.529096    0.505295    2.068609   \n",
      "112   -1.651635   -1.066110   -0.310776   -0.529572    0.801097   -0.810446   \n",
      "19     1.603039   -0.859032    0.776930    0.501488    1.895967    2.117006   \n",
      "78     1.557560   -0.862355    0.804145    1.351447    0.272718   -0.720665   \n",
      "109    1.442917   -0.698778    0.804901    0.405859    1.627476    0.773735   \n",
      "\n",
      "     cg04655481  cg04857395  cg05301188  cg05317090  ...  cg19966212  \\\n",
      "37    -1.137842    0.733058   -0.191998    0.222425  ...    2.677239   \n",
      "112   -1.178068    0.765305   -1.084208   -0.772244  ...    0.956927   \n",
      "19    -1.025275    0.723857    1.663560    1.860212  ...    2.648444   \n",
      "78    -0.992336    0.734956    1.572086    2.093754  ...    2.622459   \n",
      "109    0.431721    0.714562    1.116127    0.679836  ...    2.150078   \n",
      "\n",
      "     cg21387009  cg21397540  cg22731271  cg23171972  cg23732024  cg24675150  \\\n",
      "37     1.532021    1.009131   -0.704194    0.768709    0.019012   -1.045516   \n",
      "112    1.419548    1.347385   -0.701117    0.801085    0.122810   -0.818478   \n",
      "19     1.469987    0.967904   -0.674525   -0.263199    1.172845   -1.039419   \n",
      "78     1.501713   -0.570834   -0.695096    0.860952    1.177722    1.789698   \n",
      "109    1.482860    1.457740   -0.687868    0.584705    1.185507   -0.932201   \n",
      "\n",
      "     cg26607828  cg27214856  Death  \n",
      "37     1.795964    0.837543    0.0  \n",
      "112   -0.291138   -1.720363    1.0  \n",
      "19     2.129514    0.790021    0.0  \n",
      "78     1.914871    0.713365    1.0  \n",
      "109    1.550916    0.804947    1.0  \n",
      "\n",
      "[5 rows x 49 columns]\n",
      "\n",
      "\n",
      "Block 15:\n",
      "     cg00791868  cg01252023  cg01724917  cg02239258  cg02891728  cg03173502  \\\n",
      "23     1.080879    0.978168    0.733653    0.691880    1.017995   -0.599629   \n",
      "37     1.486384    1.125467    0.805568    1.529096    1.200949   -1.777757   \n",
      "19     1.529264    1.107286    0.776930    0.501488    1.191803   -1.951312   \n",
      "109    1.338564    1.068770    0.804901    0.405859    1.190659   -1.473291   \n",
      "72     1.153172    1.080239    0.758640    1.037153    1.175789   -1.839521   \n",
      "\n",
      "     cg03548415  cg03956042  cg03972560  cg03982355  ...  cg21397540  \\\n",
      "23    -0.389262   -0.366259   -0.609093    1.094361  ...   -0.384548   \n",
      "37     1.456615    0.610499    0.505295    2.068609  ...    1.009131   \n",
      "19     1.532449    1.815947    1.895967    2.117006  ...    0.967904   \n",
      "109    0.874082    0.672184    1.627476    0.773735  ...    1.457740   \n",
      "72     1.282064    0.245865    0.387697    1.814503  ...   -0.982370   \n",
      "\n",
      "     cg21637392  cg23690893  cg23732024  cg25591794  cg26235215  cg26607828  \\\n",
      "23     0.059198    1.371788    1.220133    1.081194    1.362835    0.603571   \n",
      "37     0.097953    1.415139    0.019012   -0.126293   -0.696888    1.795964   \n",
      "19     1.905997    1.398669    1.172845    0.933958   -0.558523    2.129514   \n",
      "109   -0.536029    1.344417    1.185507    0.023722   -0.321577    1.550916   \n",
      "72    -0.689297    1.370316    1.161663   -1.812348   -0.414957    1.982518   \n",
      "\n",
      "     cg27214856  cg27285720  Death  \n",
      "23    -0.335675    1.300777    1.0  \n",
      "37     0.837543    2.235116    0.0  \n",
      "19     0.790021    2.131888    0.0  \n",
      "109    0.804947    1.946437    1.0  \n",
      "72     0.754757    1.942509    0.0  \n",
      "\n",
      "[5 rows x 46 columns]\n",
      "\n",
      "\n",
      "Block 16:\n",
      "     cg00141845  cg00532474  cg00791868  cg03173502  cg03548415  cg03672272  \\\n",
      "95     0.129766    1.564413    1.362036    0.788027   -0.658070   -0.682219   \n",
      "126    0.456047    0.669813    1.121089    0.399742   -0.928411   -0.642880   \n",
      "113    1.467245    1.582973    0.701909    0.841022   -1.069253   -0.613268   \n",
      "138    1.314983    1.270045    1.476723   -1.517082    1.368646   -0.589540   \n",
      "19    -0.230719    1.603039    1.529264   -1.951312    1.532449    0.671111   \n",
      "\n",
      "     cg03956042  cg05317090  cg05821046  cg07195011  ...  cg16409562  \\\n",
      "95     0.871693    1.322675    1.419585   -0.617563  ...    1.318221   \n",
      "126   -0.100626   -0.611853    0.707382   -0.635226  ...   -0.332631   \n",
      "113   -0.913809   -0.707397    1.374527    2.153338  ...    0.610366   \n",
      "138    1.397628    1.633170   -0.617114   -0.617231  ...    1.280713   \n",
      "19     1.815947    1.860212    1.317170    0.289035  ...    1.318578   \n",
      "\n",
      "     cg19537719  cg19966212  cg21397540  cg23690893  cg23732024  cg26607828  \\\n",
      "95     1.074956    2.091541    0.466923    1.401018    1.111402    1.677690   \n",
      "126    0.845960   -0.515443   -0.600110    1.288062    1.130007   -0.684846   \n",
      "113    0.095117   -0.524460    0.403696    1.413717    1.068010    1.845494   \n",
      "138   -1.583171    2.467575    0.870628    1.331006    1.157421    1.452583   \n",
      "19    -0.626250    2.648444    0.967904    1.398669    1.172845    2.129514   \n",
      "\n",
      "     cg27214856  cg27285720  Death  \n",
      "95     0.808305   -0.571940    1.0  \n",
      "126   -0.136207    1.538792    0.0  \n",
      "113   -1.411562   -0.645331    1.0  \n",
      "138    0.802640    2.048224    1.0  \n",
      "19     0.790021    2.131888    0.0  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "\n",
      "\n",
      "Block 17:\n",
      "     cg00791868  cg01252023  cg01542019  cg02605007  cg02891728  cg03407547  \\\n",
      "14    -0.863518    0.938529    0.802593   -0.466524    0.105247    1.917847   \n",
      "95     1.362036    0.973841    0.857331    1.171226    1.187237    2.118630   \n",
      "19     1.529264    1.107286    0.940235    1.997354    1.191803    2.549653   \n",
      "109    1.338564    1.068770    0.947303   -0.764713    1.190659    1.066919   \n",
      "78     1.534360    1.058910    0.892293    1.939560    0.794261   -0.673870   \n",
      "\n",
      "     cg03548415  cg03672272  cg03956042  cg05317090  ...  cg21637392  \\\n",
      "14    -0.579937    1.474744    0.594975   -0.406141  ...    1.943057   \n",
      "95    -0.658070   -0.682219    0.871693    1.322675  ...    2.068865   \n",
      "19     1.532449    0.671111    1.815947    1.860212  ...    1.905997   \n",
      "109    0.874082   -0.659994    0.672184    0.679836  ...   -0.536029   \n",
      "78     1.418686   -0.806902    1.278170    2.093754  ...   -0.699153   \n",
      "\n",
      "     cg22365240  cg23690893  cg23732024  cg25591794  cg26607828  cg26814396  \\\n",
      "14    -0.488809    1.182789    0.369566    0.626744   -0.580820    0.920242   \n",
      "95     1.492244    1.401018    1.111402    0.816949    1.677690    0.928336   \n",
      "19     1.300636    1.398669    1.172845    0.933958    2.129514    1.072051   \n",
      "109    1.572370    1.344417    1.185507    0.023722    1.550916    0.877875   \n",
      "78     1.472816    1.410756    1.177722    0.918921    1.914871    1.048948   \n",
      "\n",
      "     cg27214856  cg27285720  Death  \n",
      "14     0.585066   -0.613853    1.0  \n",
      "95     0.808305   -0.571940    1.0  \n",
      "19     0.790021    2.131888    0.0  \n",
      "109    0.804947    1.946437    1.0  \n",
      "78     0.713365    2.155834    1.0  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "\n",
      "\n",
      "Block 18:\n",
      "     cg01542019  cg02891728  cg03126946  cg03173502  cg03407547  cg03672272  \\\n",
      "19     0.940235    1.191803    1.154310   -1.951312    2.549653    0.671111   \n",
      "75     0.883794   -1.096321    1.121373   -0.885237    2.080859    1.068770   \n",
      "95     0.857331    1.187237    1.102468    0.788027    2.118630   -0.682219   \n",
      "138    0.930049    1.190886    1.002485   -1.517082    2.104436   -0.589540   \n",
      "72     0.958857    1.175789    1.104156   -1.839521    2.189128   -0.935241   \n",
      "\n",
      "     cg03982355  cg06466348  cg08447324  cg08943714  ...  cg12448285  \\\n",
      "19     2.117006    1.065167   -0.619966    1.431151  ...    0.977313   \n",
      "75     1.875321   -0.198051   -0.621031   -1.214752  ...   -0.440494   \n",
      "95     1.836646    1.010096   -0.614030    0.194630  ...    0.955090   \n",
      "138    1.710321    1.082025   -0.604583   -0.387613  ...    0.986630   \n",
      "72     1.814503    1.032478   -0.615233    0.619948  ...    1.010342   \n",
      "\n",
      "     cg13073699  cg14084907  cg14684457  cg14703224  cg18179039  cg26607828  \\\n",
      "19     1.891161    0.149820    1.961235    0.514243    1.689197    2.129514   \n",
      "75    -0.774745    0.751837    1.560535   -1.429512    0.688975    0.417431   \n",
      "95     1.864911    0.671981    1.277918    1.268785    1.714854    1.677690   \n",
      "138    1.813188    0.238804    1.905522    0.540509    1.754932    1.452583   \n",
      "72     1.860329    1.031383    1.299888    1.264900    1.655496    1.982518   \n",
      "\n",
      "     cg26814396  cg27285720  Death  \n",
      "19     1.072051    2.131888    0.0  \n",
      "75     0.447908    2.145486    NaN  \n",
      "95     0.928336   -0.571940    1.0  \n",
      "138    0.884409    2.048224    1.0  \n",
      "72     0.962423    1.942509    0.0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "\n",
      "Block 19:\n",
      "     cg00861646  cg00987534  cg01208318  cg01231141  cg01542019  cg01724917  \\\n",
      "20    -0.517989   -0.794523    1.153800   -1.942886    0.825282    0.608152   \n",
      "112    0.497666   -1.066110    1.303747    1.020201    0.950616   -0.310776   \n",
      "37    -0.595715   -0.933457    1.199791   -1.506771    0.964931    0.805568   \n",
      "78     1.281737   -0.862355    1.267097    0.613536    0.892293    0.804145   \n",
      "109    0.786387   -0.698778    1.208797   -0.962819    0.947303    0.804901   \n",
      "\n",
      "     cg01891966  cg02239258  cg02367655  cg02605007  ...  cg19893929  \\\n",
      "20    -0.778393    1.365331   -0.579582   -0.919645  ...    0.969590   \n",
      "112   -0.894729   -0.529572   -0.346165    1.170853  ...    1.110297   \n",
      "37    -0.976004    1.529096    1.371025    0.639836  ...    1.136126   \n",
      "78    -0.879436    1.351447    1.142559    1.939560  ...    1.065659   \n",
      "109   -0.913114    0.405859    1.345133   -0.764713  ...    1.098824   \n",
      "\n",
      "     cg20927661  cg21387009  cg21397540  cg22731271  cg23171972  cg23732024  \\\n",
      "20    -1.107665    1.310974    0.223933   -0.694680    0.714096    0.426049   \n",
      "112   -1.052790    1.419548    1.347385   -0.701117    0.801085    0.122810   \n",
      "37    -0.944510    1.532021    1.009131   -0.704194    0.768709    0.019012   \n",
      "78     0.055998    1.501713   -0.570834   -0.695096    0.860952    1.177722   \n",
      "109    0.834370    1.482860    1.457740   -0.687868    0.584705    1.185507   \n",
      "\n",
      "     cg27214856  rs1510189  Death  \n",
      "20    -1.507384   0.101107    1.0  \n",
      "112   -1.720363   1.479927    1.0  \n",
      "37     0.837543  -1.372596    0.0  \n",
      "78     0.713365  -1.385453    1.0  \n",
      "109    0.804947   0.065866    1.0  \n",
      "\n",
      "[5 rows x 55 columns]\n",
      "\n",
      "\n",
      "Block 20:\n",
      "     SLC12A8.84561  cg06363129  cg08343075  cg15127250  cg15417249  \\\n",
      "47        0.136854    0.707587   -0.784820    0.843700    1.657908   \n",
      "142      -0.353473   -0.202415   -0.244963   -0.111911    0.700848   \n",
      "28       -0.052952    0.412377   -1.016021    1.091785    1.658908   \n",
      "123      -0.285955    0.447599   -1.140305    0.845952    1.578016   \n",
      "92       -3.888589    0.705239   -1.246059    1.087658    1.750485   \n",
      "\n",
      "     cg17758673  cg19878482  cg19966212  cg24527636  Death  \n",
      "47    -1.034908   -1.022455    2.298720   -1.627036    1.0  \n",
      "142   -0.262084   -1.025802    1.262561   -0.707031    1.0  \n",
      "28    -0.985928   -0.961574    2.523174   -2.035858    0.0  \n",
      "123   -1.002836   -1.264119    2.199023   -2.087947    1.0  \n",
      "92    -1.302037   -1.160192    2.485023   -1.582527    0.0  \n",
      "\n",
      "\n",
      "Block 21:\n",
      "     cg01724917  cg01891966  cg03604774  cg03982355  cg04797323  cg08347500  \\\n",
      "62    -1.992750   -0.706487    0.869473    1.957886    0.464167    2.460187   \n",
      "94    -2.171210    1.991179    0.759659    1.935329    1.580801    1.046872   \n",
      "152   -0.478888   -0.932670    0.808382    1.399838    0.771732    0.823490   \n",
      "3     -1.675189    0.659081    0.863268   -0.845328    1.116049    1.601944   \n",
      "2      0.729542    0.211838    0.792887    1.574619    0.224135   -0.619034   \n",
      "\n",
      "     cg09462281  cg10980495  cg11021222  cg12448285  ...  cg16306870  \\\n",
      "62     0.823947   -0.545805   -0.947429   -1.394997  ...    0.936044   \n",
      "94     0.923706    0.464629   -1.460590    1.030239  ...    0.947804   \n",
      "152    0.898483    0.735115    0.250779    1.057063  ...   -1.736562   \n",
      "3      0.883879    1.042460   -0.071285    0.968579  ...    0.482670   \n",
      "2      0.883808    1.643683    1.456369    0.997801  ...    0.684899   \n",
      "\n",
      "     cg18482303  cg19537719  cg20747577  cg21397540  cg21637392  cg25020666  \\\n",
      "62    -0.616302    1.113014   -0.582194   -1.138992   -0.778743    1.174650   \n",
      "94     0.377936    1.087988    2.111263   -1.007918   -0.774005    1.106955   \n",
      "152    1.084702    1.109735   -0.613610    0.872306   -0.752441    1.122534   \n",
      "3     -1.252570    1.073974    0.785885   -0.689104   -0.757984    1.006855   \n",
      "2      0.956577    0.951839   -0.630262   -0.925786    0.979888    1.134222   \n",
      "\n",
      "     cg25145765  cg26235215  Death  \n",
      "62     2.270130    2.603480    1.0  \n",
      "94     2.078867    2.483962    1.0  \n",
      "152    2.117076    2.475063    0.0  \n",
      "3      1.834183    2.167358    0.0  \n",
      "2      0.815447   -0.430771    1.0  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "\n",
      "Block 22:\n",
      "     CT45A3.441519  GPR142.350383  ZCCHC12.170261  cg01089498  cg01515515  \\\n",
      "69        1.976512      -0.963051       -1.292464   -0.755035   -1.380611   \n",
      "120       2.087717      -0.963051       -1.292464   -0.124870   -1.325512   \n",
      "129       1.976984       0.826108        0.944290   -0.772813   -0.894936   \n",
      "49        1.917151       0.860364       -1.292464   -0.786061   -1.331073   \n",
      "119       1.794050       0.853183        0.619221   -0.765859    0.088368   \n",
      "\n",
      "     cg02137956  cg05915866  cg06443533  cg07110356  cg08173263  ...  \\\n",
      "69    -1.093566   -1.355208   -0.900466    1.163655   -0.767837  ...   \n",
      "120   -0.437580   -1.282283   -0.513722    1.183235   -0.805941  ...   \n",
      "129   -1.117600   -1.355336    0.128892   -1.424043   -0.801302  ...   \n",
      "49    -0.101646   -1.279027   -0.891557   -1.175289   -0.851859  ...   \n",
      "119   -0.938255   -1.332868   -0.893891   -1.266589   -0.320791  ...   \n",
      "\n",
      "     cg21637392  cg22222281  cg23530553  cg24105081  cg25496181  cg25801976  \\\n",
      "69     1.001635   -1.321523   -1.279894   -1.315774   -0.559803   -0.651613   \n",
      "120   -0.679959   -1.366927    0.163715   -1.352509   -0.543202   -1.110142   \n",
      "129    0.855400   -1.412466   -1.540623   -1.299238   -0.539442   -1.162469   \n",
      "49    -0.552660   -1.134652   -1.370930   -1.097691   -0.533896   -0.828997   \n",
      "119    1.408995    0.110099   -1.323209   -0.992513   -0.441212   -0.942062   \n",
      "\n",
      "     cg26376241  cg26814396  rs1510189  Death  \n",
      "69    -0.783611   -1.610877   0.011116    0.0  \n",
      "120   -0.709385   -1.376084   0.018035    0.0  \n",
      "129   -0.753830   -1.287979   1.503017    0.0  \n",
      "49    -0.764982   -1.482858   1.504696    1.0  \n",
      "119   -0.784523   -0.564961   0.043246    1.0  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "\n",
      "\n",
      "Block 23:\n",
      "     cg04268950  cg07127410  cg07816074  cg11204139  cg15241084  cg17216243  \\\n",
      "127    1.629817    1.163003   -2.258308    1.223537    0.836374    0.430290   \n",
      "154   -0.816465   -0.791083    0.630738    1.008213   -1.477587    0.722873   \n",
      "131    0.486347    1.187434   -1.836704    1.111624    1.119601   -0.824808   \n",
      "143   -0.390395    0.534197   -1.810401   -0.572187    0.887632    0.924783   \n",
      "34    -0.096834    1.132699   -2.072760    1.178309    0.951884   -0.297726   \n",
      "\n",
      "     cg21375204  cg23840053  cg24199203  cg25685359  cg25884399  Death  \n",
      "127    1.198941    0.009904    0.934374   -1.222061    1.744061    1.0  \n",
      "154    1.113595   -0.907498    0.743247   -1.142244    0.977366    NaN  \n",
      "131    0.907736   -0.259464    0.899481    0.423564    0.782036    0.0  \n",
      "143    1.158501   -0.706206    0.780629    1.139106    0.730030    1.0  \n",
      "34    -0.961130   -0.646270    0.853744    0.824139   -1.122711    1.0  \n",
      "\n",
      "\n",
      "Block 24:\n",
      "     cg01089498  cg02601475  cg03956042  cg05206633  cg05301188  cg05709437  \\\n",
      "69    -0.755035   -1.489239   -1.056458   -0.911416   -1.108553   -1.529899   \n",
      "118   -0.384495   -0.616767   -0.306342    0.750202    0.079742   -0.658181   \n",
      "79    -0.770628   -1.268783   -0.958298    0.971338    0.029531   -1.283236   \n",
      "11    -0.780971   -1.446127   -1.073363    0.292566   -0.527816   -1.005649   \n",
      "49    -0.786061   -1.353372   -1.090064   -1.224662   -1.007329   -1.416262   \n",
      "\n",
      "     cg05915866  cg06443533  cg07110356  cg08347500  ...  cg18596381  \\\n",
      "69    -1.355208   -0.900466    1.163655   -0.643138  ...   -0.493245   \n",
      "118    0.090560   -0.662462    0.244403   -0.345100  ...    1.013429   \n",
      "79     0.094015   -0.859535    0.836096   -0.646933  ...    0.344896   \n",
      "11    -0.840401   -0.891399   -0.170602   -0.133405  ...   -0.788164   \n",
      "49    -1.279027   -0.891557   -1.175289   -0.632680  ...   -0.863981   \n",
      "\n",
      "     cg18786623  cg20300129  cg23531049  cg24123198  cg25801976  cg26147845  \\\n",
      "69    -0.288249   -1.187250   -0.002431   -1.726423   -0.651613   -0.954546   \n",
      "118   -0.012015    0.181906   -0.304917   -0.381854    0.863786   -0.971957   \n",
      "79    -1.560831    0.425040   -1.658983   -1.468652    0.385469   -0.891877   \n",
      "11     1.138739   -1.312575   -1.583010   -0.936844   -0.765486   -0.942988   \n",
      "49    -1.197740   -0.215622   -1.166170   -0.842274   -0.828997   -0.833600   \n",
      "\n",
      "     cg26376241  hsa.mir.1248  Death  \n",
      "69    -0.783611      0.254422    0.0  \n",
      "118   -0.383795     -0.059416    1.0  \n",
      "79    -0.778522     -4.130658    1.0  \n",
      "11    -0.743258     -0.306302    1.0  \n",
      "49    -0.764982      0.111155    1.0  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "\n",
      "\n",
      "Block 25:\n",
      "     cg02367655  cg08698943  cg14511923  cg16651537  cg19317211  cg24199203  \\\n",
      "19     1.868286   -0.621943    1.450391    0.495372   -0.075864    0.861108   \n",
      "156   -0.138188   -0.411795    1.315475    0.868212   -0.508235    0.695498   \n",
      "78     1.142559   -0.919849    1.464141    0.475926    0.027054    0.872702   \n",
      "72     1.936252   -0.821063    1.456483    0.550038   -1.014175    0.853326   \n",
      "109    1.345133   -0.497279    1.489641   -0.035032    0.414334    0.866046   \n",
      "\n",
      "     cg25884399  Death  \n",
      "19     1.653733    0.0  \n",
      "156   -0.052599    1.0  \n",
      "78     1.419762    1.0  \n",
      "72     1.659932    0.0  \n",
      "109    1.600324    1.0  \n",
      "\n",
      "\n",
      "Block 26:\n",
      "     cg02734358  cg09147140  cg12232731  cg13241645  cg15241084  cg21375204  \\\n",
      "131    1.080393    0.168391   -0.600576   -0.179891    1.119601    0.907736   \n",
      "149    0.436790   -0.600330   -1.360176    0.361326    0.994749    1.082460   \n",
      "143    0.891339    0.561533    1.195640    0.872946    0.887632    1.158501   \n",
      "59    -1.318300    0.365863   -0.142327   -0.883759    0.284240    1.089151   \n",
      "12    -1.114973   -0.830641   -0.888430   -0.274012   -1.052047    1.210381   \n",
      "\n",
      "     cg23665778  Death  \n",
      "131   -0.548283    0.0  \n",
      "149   -0.542622    0.0  \n",
      "143   -0.567709    1.0  \n",
      "59    -0.496518    NaN  \n",
      "12    -0.602453    1.0  \n",
      "\n",
      "\n",
      "Block 27:\n",
      "     cg12382846  cg13481969  cg14661139  cg18413830  cg18596381  cg23042151  \\\n",
      "69    -2.044258   -0.953766   -1.350714   -1.539946   -0.493245   -1.311213   \n",
      "115    0.614628   -0.688131   -0.361884   -0.586068   -0.902150    0.671318   \n",
      "49    -0.532613   -0.955907   -0.233405   -1.282727   -0.863981    1.131294   \n",
      "129   -0.373664   -0.954865   -1.623748   -1.473531   -0.868170    0.999950   \n",
      "54    -0.920878   -0.934611   -1.229982   -1.467677   -0.855754   -0.399369   \n",
      "\n",
      "     cg23415434  Death  \n",
      "69    -1.608803    0.0  \n",
      "115   -0.009479    0.0  \n",
      "49     1.141801    1.0  \n",
      "129   -0.345959    0.0  \n",
      "54    -0.366678    1.0  \n",
      "\n",
      "\n",
      "Block 28:\n",
      "     cg00922748  cg00997251  cg01992935  cg02632490  cg02940147  cg03548415  \\\n",
      "96     0.727339    0.647456    0.201978    0.836322   -0.645324   -0.358419   \n",
      "127    1.050593    0.665382    0.809194    0.895822    0.850033    0.711931   \n",
      "131    0.998505    0.977973    0.669750    0.917847   -1.272445   -0.151531   \n",
      "2      0.971947    0.909386    0.633620    0.890841   -0.339199    1.434109   \n",
      "46     0.597404    1.033972    0.271090    0.866003   -0.691668    1.353697   \n",
      "\n",
      "     cg03604774  cg03883572  cg07127410  cg08751352  ...  cg15288800  \\\n",
      "96     0.531107    0.621893    0.374019   -0.687831  ...    0.495355   \n",
      "127    0.819144    1.585297    1.163003   -0.901233  ...   -0.212902   \n",
      "131    0.799713    1.284596    1.187434   -0.809338  ...   -0.021401   \n",
      "2      0.792887    1.433756    1.403261   -0.749926  ...   -0.211587   \n",
      "46     0.613184    1.096348    1.088752   -0.827449  ...    1.080466   \n",
      "\n",
      "     cg16409562  cg16729415  cg17763019  cg19317211  cg20732787  cg23528247  \\\n",
      "96     0.994269   -0.751402    0.887057    0.926734    0.973783    0.499890   \n",
      "127    1.207946   -0.885044   -1.108814    1.811847    0.884705   -0.808366   \n",
      "131    1.335003   -0.879484   -0.778401    1.611575    0.785441    1.398703   \n",
      "2      1.314695    1.648089    0.691792   -0.303111    1.094109    1.216484   \n",
      "46     1.274501   -0.847124   -1.313040    0.844525    1.071535    1.326964   \n",
      "\n",
      "     cg23665778  cg26580869  Death  \n",
      "96    -0.417511    0.579953    0.0  \n",
      "127   -0.545620    1.416434    1.0  \n",
      "131   -0.548283    1.279558    0.0  \n",
      "2     -0.595402    1.218566    1.0  \n",
      "46    -0.555786    0.876531    1.0  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "\n",
      "Block 29:\n",
      "     cg00610991  cg03883572  cg03972560  cg04268950  cg04857395  cg07127410  \\\n",
      "46     1.426997    1.096348   -0.744366   -0.404473    0.646851    1.088752   \n",
      "147   -0.481596    0.300641   -0.496351   -0.899602    0.642485    1.061573   \n",
      "128   -1.237071    1.441847    0.077943   -0.425488    0.721191    1.243973   \n",
      "117    0.260229    1.348439    1.901626   -1.186272    0.757120    1.470855   \n",
      "50     0.172846    1.510108   -1.119150    0.376589   -0.659791    1.334412   \n",
      "\n",
      "     cg09975576  cg11204139  cg17763019  cg19893929  cg23690893  cg25145765  \\\n",
      "46     0.920922    1.050654   -1.313040    0.882190    1.161413   -0.833391   \n",
      "147   -0.436656    0.966258   -0.352436    0.266275    0.588247   -0.735036   \n",
      "128    0.966369    1.203792    0.538336    1.028264    1.296444   -0.271023   \n",
      "117    0.942878    1.109242    0.636493    0.415951    1.262892   -0.836065   \n",
      "50     0.895876    0.747183    0.384256   -0.852121    1.017186   -0.513633   \n",
      "\n",
      "     cg26580869  Death  \n",
      "46     0.876531    1.0  \n",
      "147   -0.013350    1.0  \n",
      "128    1.172878    0.0  \n",
      "117    1.199851    0.0  \n",
      "50     1.025108    1.0  \n",
      "\n",
      "\n",
      "Block 30:\n",
      "    CT45A3.441519  ZCCHC12.170261  cg00141845  cg00997251  cg02137956  \\\n",
      "1       -0.544615       -1.292464   -0.136342   -0.953590   -0.930652   \n",
      "32       1.855737       -1.292464    1.023680   -0.802293   -1.291647   \n",
      "69       1.976512       -1.292464   -1.606479   -0.922070   -1.093566   \n",
      "11       1.901415       -1.292464    1.215479   -1.301010   -1.221979   \n",
      "74       2.004883        0.685439   -1.401252   -1.299094   -1.285550   \n",
      "\n",
      "    cg03604774  cg05206633  cg05301188  cg05445326  cg05709437  ...  \\\n",
      "1     0.648902    0.302846    0.399168    0.727277   -0.333504  ...   \n",
      "32    0.656780   -1.673871    1.510427    0.163594    0.069534  ...   \n",
      "69   -1.634985   -0.911416   -1.108553    0.678798   -1.529899  ...   \n",
      "11    0.310597    0.292566   -0.527816    0.753872   -1.005649  ...   \n",
      "74    0.231159   -0.326682   -1.016888    0.669231   -0.940749  ...   \n",
      "\n",
      "    cg20793665  cg21201401  cg23075364  cg24123198  cg25496181  cg25801976  \\\n",
      "1     0.763147   -0.783595    0.868242   -0.865517   -0.470906    0.549854   \n",
      "32    0.146652   -0.747452    0.885958   -1.686188   -0.303870    1.149121   \n",
      "69   -2.000094   -0.789509   -1.761869   -1.726423   -0.559803   -0.651613   \n",
      "11   -0.961115   -0.794135   -1.759045   -0.936844   -0.542614   -0.765486   \n",
      "74   -0.965739   -0.778425   -1.735862   -1.708287   -0.454578   -0.392778   \n",
      "\n",
      "    cg26147845  cg27246129  hsa.mir.1248  Death  \n",
      "1    -0.914937   -1.290016      0.675597    1.0  \n",
      "32   -0.945677   -0.769799      0.664398    1.0  \n",
      "69   -0.954546   -1.408028      0.254422    0.0  \n",
      "11   -0.942988   -0.801492     -0.306302    1.0  \n",
      "74   -0.944498   -1.094450     -4.130658    1.0  \n",
      "\n",
      "[5 rows x 46 columns]\n",
      "\n",
      "\n",
      "Block 31:\n",
      "     cg00922748  cg00997251  cg01252023  cg01992935  cg02239258  cg02632490  \\\n",
      "131    0.998505    0.977973    0.973064    0.669750    1.491954    0.917847   \n",
      "150    0.950499    0.984409   -0.859869    0.782315    1.483684    0.927249   \n",
      "127    1.050593    0.665382    0.952749    0.809194    1.442246    0.895822   \n",
      "153    1.026729    0.118664    1.073733    0.803422    0.176813    0.921197   \n",
      "128    1.037589    1.116965    1.003003   -0.006983    0.765470    0.962269   \n",
      "\n",
      "     cg02940147  cg03604774  cg03883572  cg03956042  ...  cg16651537  \\\n",
      "131   -1.272445    0.799713    1.284596   -0.701606  ...    0.703980   \n",
      "150    1.003414    0.670776    1.488462    0.471570  ...    0.565238   \n",
      "127    0.850033    0.819144    1.585297   -1.021170  ...   -0.999565   \n",
      "153   -0.784046    0.528140    1.498131    1.239584  ...    1.352351   \n",
      "128   -0.753666    0.696642    1.441847   -0.035620  ...    2.080180   \n",
      "\n",
      "     cg16729415  cg17763019  cg18482303  cg19317211  cg21201401  cg21637392  \\\n",
      "131   -0.879484   -0.778401   -0.269970    1.611575    1.470154   -0.637368   \n",
      "150    1.093656    0.707933   -1.847537    1.715368   -0.581097   -0.758444   \n",
      "127   -0.885044   -1.108814    0.303763    1.811847    1.242077   -0.765072   \n",
      "153   -0.641149    0.624375    1.007829    1.855185    1.880478    2.140925   \n",
      "128    0.022783    0.538336    1.073490   -0.294153    1.812683    0.547001   \n",
      "\n",
      "     cg23171972  cg26580869  Death  \n",
      "131    0.844186    1.279558    0.0  \n",
      "150   -0.022529    0.742796    0.0  \n",
      "127    0.821201    1.416434    1.0  \n",
      "153    0.195507    1.302802    1.0  \n",
      "128    0.576867    1.172878    0.0  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "\n",
      "\n",
      "Block 32:\n",
      "     cg00715696  cg05915866  cg06443533  cg07110356  cg12835118  cg17104258  \\\n",
      "4     -0.184828    1.490965    1.208449   -1.176301   -0.613730    0.776235   \n",
      "45    -1.186890    0.636606    0.705822   -1.459295    2.286226    2.115976   \n",
      "82    -1.216158    0.228580    0.294169   -1.412721    0.872912    1.604705   \n",
      "7     -1.077537    0.155221   -0.897694   -0.848251    1.635497    1.648115   \n",
      "133   -1.151240    1.631988    2.124268   -1.331677    2.290080    1.945923   \n",
      "\n",
      "     Death  \n",
      "4      1.0  \n",
      "45     0.0  \n",
      "82     1.0  \n",
      "7      1.0  \n",
      "133    NaN  \n",
      "\n",
      "\n",
      "Block 33:\n",
      "     cg01252023  cg03548415  cg03972560  cg04268950  cg07127410  cg07195011  \\\n",
      "6     -0.040609   -1.050081   -0.977960    0.035275    1.048311   -0.566284   \n",
      "63     0.773527   -0.979070    0.989399    1.380562    1.304496   -0.352313   \n",
      "128    1.003003    0.647631    0.077943   -0.425488    1.243973    1.852118   \n",
      "153    1.073733   -0.975759   -1.119342   -0.457537    1.437647    0.537099   \n",
      "2      1.030497    1.434109    0.079689    0.637103    1.403261    1.314899   \n",
      "\n",
      "     cg08183317  cg08196968  cg08751352  cg09232555  ...  cg19893929  \\\n",
      "6      1.054561   -0.470150   -0.652196   -0.915082  ...   -1.035924   \n",
      "63     1.512043    1.288104   -0.913127   -0.928317  ...   -0.747476   \n",
      "128    1.939614    1.854407   -0.858828    0.175162  ...    1.028264   \n",
      "153    1.971834    2.088461   -0.692038   -0.828868  ...    1.121602   \n",
      "2      1.199938    1.981175   -0.749926   -0.679744  ...    0.946763   \n",
      "\n",
      "     cg21397540  cg21637392  cg22365240  cg23528247  cg25591794  cg26235215  \\\n",
      "6     -0.927038   -0.505818    0.701928    0.098043    0.379553   -0.585554   \n",
      "63    -0.785283    1.723360    1.211926    0.335056    0.890836   -0.574074   \n",
      "128   -0.804710    0.547001    1.501759    0.956567    0.935048    0.543565   \n",
      "153    1.414191    2.140925    1.564583   -0.536247    0.127013   -0.299161   \n",
      "2     -0.925786    0.979888    1.474767    1.216484    0.977411   -0.430771   \n",
      "\n",
      "     cg26580869  hsa.mir.153.2  Death  \n",
      "6     -0.959119       0.301277    1.0  \n",
      "63     0.312416       0.988315    1.0  \n",
      "128    1.172878       0.592735    0.0  \n",
      "153    1.302802       0.646137    1.0  \n",
      "2      1.218566       1.194475    1.0  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "\n",
      "Block 34:\n",
      "      CA4.762  IGSF5.150084  SHANK1.50944  SIX3.6496  SLC35F4.341880  \\\n",
      "134  0.732694     -0.213493      0.493713   0.698752        0.416465   \n",
      "136  1.089742     -0.213493     -1.703769  -0.849610        0.882052   \n",
      "18   0.531133     -0.213493      0.888550  -0.849610       -1.529779   \n",
      "8    0.555288     -0.213493      0.767678  -0.849610       -1.529779   \n",
      "29   0.961451     -0.213493      0.370008   0.713719        0.435278   \n",
      "\n",
      "     cg00532474  cg01423695  cg01517680  cg03060555  cg03672272  ...  \\\n",
      "134   -1.414407   -2.063143   -0.048571   -0.155566   -0.430480  ...   \n",
      "136   -1.519066   -0.609661   -0.108351   -0.562470   -0.666642  ...   \n",
      "18    -1.545978   -1.605218   -0.676679   -0.357499   -0.768654  ...   \n",
      "8     -0.674374   -2.368593   -1.846155   -1.131187   -0.756995  ...   \n",
      "29    -0.522239   -2.085366    0.851084   -0.551482   -0.352146  ...   \n",
      "\n",
      "     cg12079322  cg12382846  cg12744859  cg13298116  cg15085883  cg15145341  \\\n",
      "134   -0.550199    0.792764    0.968215   -1.531183   -1.007596   -1.211001   \n",
      "136   -0.648268    0.862633   -0.844308   -0.242385   -0.932222   -1.198897   \n",
      "18     0.030881   -1.015476   -0.907910   -1.804910   -0.919811    0.273314   \n",
      "8     -0.640357    0.845118    0.530026   -1.626408   -0.848122    0.492204   \n",
      "29    -0.368986    0.736299    0.668276   -0.602420   -1.007515   -1.306597   \n",
      "\n",
      "     cg18786623  cg20518446  cg21805940  Death  \n",
      "134   -0.811986    0.003398    0.102963    1.0  \n",
      "136   -1.572472    0.498375   -0.207447    1.0  \n",
      "18    -1.463990   -1.076253   -1.526300    1.0  \n",
      "8     -1.550702   -0.310343   -0.316057    0.0  \n",
      "29    -0.890559   -0.878085    0.310870    1.0  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "\n",
      "\n",
      "Block 35:\n",
      "       C3.718  GLDN.342035  cg00715696  cg01089498  cg01538731  cg02137956  \\\n",
      "15  -0.363942    -0.069494   -1.219127    0.428037    1.093415   -1.315090   \n",
      "32  -0.360714     1.363718    1.013661    2.289813    1.009238   -1.291647   \n",
      "141 -3.737307    -3.500787   -0.339843    0.316107    0.024445   -1.139024   \n",
      "104  0.465766     0.537860    0.645580   -0.369777    1.040299   -1.266807   \n",
      "144 -0.157035    -0.005004    0.198423   -0.759992    1.146541   -1.095649   \n",
      "\n",
      "     cg02239258  cg04287574  cg04821520  cg06716182  ...  cg23762517  \\\n",
      "15     0.146107    1.264954   -0.682759    1.568382  ...    1.028905   \n",
      "32     0.598677    1.293447    0.770331    1.197444  ...    0.978078   \n",
      "141    1.340738    1.146724    0.164576    1.437969  ...    0.997088   \n",
      "104    0.050278    0.039640   -0.537732    1.591536  ...    0.984195   \n",
      "144    0.725268   -1.597873   -0.637079    1.620647  ...    0.984170   \n",
      "\n",
      "     cg24123198  cg24862510  cg25496181  cg25605731  cg25801976  cg27246129  \\\n",
      "15    -1.426336    2.467528    2.714811    0.769878   -0.255233    1.175223   \n",
      "32    -1.686188   -0.633774   -0.303870    0.776769    1.149121   -0.769799   \n",
      "141    0.136453    2.019365    2.401606    0.867010    1.813843   -0.234373   \n",
      "104   -1.642981    1.510736    2.453410   -1.016840   -1.081491   -0.186154   \n",
      "144   -1.663843   -0.683163   -0.547039    0.303495   -0.323682   -1.496993   \n",
      "\n",
      "     hsa.mir.1248  hsa.mir.155  Death  \n",
      "15       0.525790    -0.731424    1.0  \n",
      "32       0.664398     1.716530    1.0  \n",
      "141      0.661326     1.647528    0.0  \n",
      "104      0.681984    -0.981547    1.0  \n",
      "144     -0.096513    -0.595099    1.0  \n",
      "\n",
      "[5 rows x 47 columns]\n",
      "\n",
      "\n",
      "Block 36:\n",
      "     ATP10B.23120   CA4.762  PISRT1.140464  SHANK1.50944  SIX3.6496  \\\n",
      "18      -2.079059  0.531133      -0.198007      0.888550  -0.849610   \n",
      "29      -2.079059  0.961451      -0.198007      0.370008   0.713719   \n",
      "134     -2.079059  0.732694      -0.198007      0.493713   0.698752   \n",
      "8        0.174789  0.555288      -0.198007      0.767678  -0.849610   \n",
      "49      -2.079059 -1.340314      -0.198007     -1.163416  -0.849610   \n",
      "\n",
      "     SYCE1.93426  TM7SF4.81501  UGT2B7.7364  cg01231141  cg01423695  ...  \\\n",
      "18     -1.252558      0.755411    -0.257913   -0.245146   -1.605218  ...   \n",
      "29     -1.252558     -1.173750    -0.257913    0.564606   -2.085366  ...   \n",
      "134     0.782036      0.693299    -0.257913    0.673729   -2.063143  ...   \n",
      "8       0.804646      0.667798    -0.257913   -0.162062   -2.368593  ...   \n",
      "49     -1.252558      0.635124    -0.257913   -1.869908    0.140626  ...   \n",
      "\n",
      "     cg18786623  cg19047660  cg20910008  cg23075364  cg23665778  cg24757160  \\\n",
      "18    -1.463990   -0.681786   -0.897128   -0.504588   -0.353579   -0.650317   \n",
      "29    -0.890559    1.257874   -0.362675    0.935076    0.130631   -0.316333   \n",
      "134   -0.811986   -0.718080    1.140689    0.681748   -0.607488   -0.312689   \n",
      "8     -1.550702   -0.267361    0.950431   -0.284803   -0.398968   -0.703588   \n",
      "49    -1.197740   -0.802550   -0.749087   -1.364514   -0.626368   -0.949311   \n",
      "\n",
      "     cg26376241  cg26814396  cg27246129  Death  \n",
      "18    -0.682423   -1.638133    0.357087    1.0  \n",
      "29    -0.244014    0.110583    1.342725    1.0  \n",
      "134   -0.597064   -1.557349   -0.055990    1.0  \n",
      "8     -0.473778   -1.427397   -0.130435    0.0  \n",
      "49    -0.764982   -1.482858   -1.344097    1.0  \n",
      "\n",
      "[5 rows x 53 columns]\n",
      "\n",
      "\n",
      "Block 37:\n",
      "     cg00141845  cg01252023  cg01542019  cg01724917  cg02891728  cg03060555  \\\n",
      "30     1.258729    0.196595   -0.240209   -1.604451    0.207728    0.502948   \n",
      "113    1.467245    1.033198    0.811456    0.831337    1.196086    1.347919   \n",
      "95     0.129766    0.973841    0.857331    0.726256    1.187237    1.416353   \n",
      "138    1.314983    1.018798    0.930049    0.748479    1.190886    1.285707   \n",
      "19    -0.230719    1.107286    0.940235    0.776930    1.191803    1.496235   \n",
      "\n",
      "     cg03254465  cg03407547  cg03604774  cg03672272  ...  cg21397540  \\\n",
      "30     1.088347    0.726530   -1.923311   -0.859139  ...    1.951671   \n",
      "113    0.972548    1.981834    0.710785   -0.613268  ...    0.403696   \n",
      "95     1.073249    2.118630    0.665635   -0.682219  ...    0.466923   \n",
      "138    0.935775    2.104436    0.784537   -0.589540  ...    0.870628   \n",
      "19     0.732380    2.549653    0.792023    0.671111  ...    0.967904   \n",
      "\n",
      "     cg23075364  cg23422268  cg23665778  cg24163242  cg25145765  cg26607828  \\\n",
      "30     0.955290    0.900969   -0.610474   -0.280804    1.578360    1.418763   \n",
      "113    0.988438    1.286581    0.311521   -0.968227    0.238731    1.845494   \n",
      "95     0.971751    1.361355   -0.519570    1.320393   -0.207781    1.677690   \n",
      "138    0.882566    1.043354   -0.487800    1.797731   -0.641508    1.452583   \n",
      "19     0.963428    1.249835   -0.531568    1.753701   -0.600084    2.129514   \n",
      "\n",
      "     cg27280688  cg27285720  Death  \n",
      "30     1.130553    0.208338    NaN  \n",
      "113    1.151150   -0.645331    1.0  \n",
      "95     1.098647   -0.571940    1.0  \n",
      "138    1.153829    2.048224    1.0  \n",
      "19     1.250230    2.131888    0.0  \n",
      "\n",
      "[5 rows x 44 columns]\n",
      "\n",
      "\n",
      "Block 38:\n",
      "     cg03564727  cg05551825  cg09122035  cg14171514  cg20513976  Death\n",
      "3      1.508355   -0.843717    0.590307    1.628707   -0.728792    0.0\n",
      "33    -0.553223   -0.801483   -1.709666   -0.528071   -0.574637    1.0\n",
      "133   -1.193972   -0.890968    1.020323    1.026865   -0.916360    NaN\n",
      "27    -1.200606   -0.921143    1.015439   -0.128632    2.109971    0.0\n",
      "28    -1.079588   -0.920563    1.046294   -1.031680   -0.892269    0.0\n",
      "\n",
      "\n",
      "Block 39:\n",
      "    cg04858586  cg14603098  cg27072996  cg27404676  Death\n",
      "7     1.387234    2.080329   -0.714067    1.376629    1.0\n",
      "21   -0.338902    1.989883   -0.728629    1.162092    0.0\n",
      "55    1.293305    1.860706   -0.689164    0.122125    0.0\n",
      "93    0.696746    0.948600   -0.675931    1.025050    0.0\n",
      "44    1.112423    1.945517   -0.652642    1.030191    0.0\n",
      "\n",
      "\n",
      "Block 40:\n",
      "    cg07397033  cg07816074  cg15241084  cg15916399  Death\n",
      "56    1.516380   -2.106981    1.207416   -0.124217    0.0\n",
      "61    1.344851   -0.768337   -0.098104    0.721374    0.0\n",
      "43   -0.588457   -2.231658    0.914454    0.978408    1.0\n",
      "97    0.883155   -1.415169    0.818223   -0.350941    1.0\n",
      "66   -1.628668   -2.216623   -1.740219    1.521464    1.0\n",
      "\n",
      "\n",
      "Block 41:\n",
      "     cg02734358  cg11697194  cg12232731  cg26806779  Death\n",
      "42    -0.469814   -0.895266    0.496342    1.177154    1.0\n",
      "127    0.712513    1.329203   -1.377659    0.839284    1.0\n",
      "131    1.080393    1.107147   -0.600576   -0.944298    0.0\n",
      "76    -1.452672    1.089018    0.992388   -0.883838    0.0\n",
      "53    -0.025455   -1.322544    0.515934   -1.096536    1.0\n",
      "\n",
      "\n",
      "Block 42:\n",
      "     cg14661139  cg14967804  cg23884076  Death\n",
      "39     0.016510   -1.300672    1.749495    1.0\n",
      "108    0.021911   -0.886308    0.974769    1.0\n",
      "68    -0.700902   -1.272710    1.470928    1.0\n",
      "58    -0.900335    0.306886    1.367370    1.0\n",
      "47     0.099819   -1.275128    1.829997    1.0\n",
      "\n",
      "\n",
      "Block 43:\n",
      "     cg01231141  cg13241645  cg23665778  Death\n",
      "76     0.636859    0.241351   -0.607165    0.0\n",
      "131    0.911541   -0.179891   -0.548283    0.0\n",
      "127    0.803490   -0.761145   -0.545620    1.0\n",
      "34    -1.852078   -0.109137   -0.587778    1.0\n",
      "63     0.454837   -1.016077   -0.574648    1.0\n",
      "\n",
      "\n",
      "Block 44:\n",
      "     cg08698943  cg22731271  cg23171972  Death\n",
      "16     1.031798   -0.664560   -0.874517    1.0\n",
      "63     1.501128    0.189186   -1.883632    1.0\n",
      "153    1.630158   -0.639540    0.195507    1.0\n",
      "128    1.771071   -0.558939    0.576867    0.0\n",
      "2      1.757391    1.092447    0.799553    1.0\n",
      "\n",
      "\n",
      "Block 45:\n",
      "    CHAC1.79094  CYP17A1.1586  FAM170B.170370  LPO.4025  MYBPH.4608  \\\n",
      "40    -0.023480      0.934142       -0.421134  0.435907   -1.229689   \n",
      "74     0.176440      0.873854        2.525706 -0.761985   -1.229689   \n",
      "11     0.146303     -0.978451       -0.421134 -0.135237    0.471172   \n",
      "\n",
      "    PISRT1.140464  S100A9.6280  SHANK1.50944  ST8SIA1.6489  TNNI2.7136  ...  \\\n",
      "40      -0.198007    -1.664744     -2.078738      0.206291    0.263119  ...   \n",
      "74      -0.198007    -0.833756     -1.478490     -0.459091    0.192724  ...   \n",
      "11      -0.198007    -1.760611     -0.766330     -0.394498   -2.460742  ...   \n",
      "\n",
      "    cg25988214  cg26147845  cg26376241  cg26607828  cg27246129  cg27285720  \\\n",
      "40   -1.155168   -0.926979   -0.662713   -0.731336    0.376629    0.304292   \n",
      "74   -1.211305   -0.944498   -0.760182   -0.761563   -1.094450    0.242272   \n",
      "11   -1.204271   -0.942988   -0.743258   -0.764704   -0.801492    0.317866   \n",
      "\n",
      "    hsa.mir.100  hsa.mir.1248  hsa.mir.181c  Death  \n",
      "40     0.022943      0.058157     -0.199600    0.0  \n",
      "74    -1.478538     -4.130658     -1.124181    1.0  \n",
      "11    -0.040909     -0.306302     -0.430224    1.0  \n",
      "\n",
      "[3 rows x 94 columns]\n",
      "\n",
      "\n",
      "Block 46:\n",
      "     ACTL9.284382    AR.367  ATP12A.479  BCL8.606  CASP5.838  CXADRP3.440224  \\\n",
      "49       2.366483 -0.033747   -0.870808  0.161232   0.417848       -0.367864   \n",
      "106     -0.399636  0.098714   -0.870808  0.633130   0.860079       -0.367864   \n",
      "129     -0.399636 -0.185845   -0.870808 -0.098902  -1.635635       -0.367864   \n",
      "\n",
      "     ETV7.51513  GPR142.350383  HIST1H3B.8358  HRASLS.57110  ...  cg22222281  \\\n",
      "49     0.594552       0.860364      -0.094444     -0.488779  ...   -1.134652   \n",
      "106    0.155236      -0.963051       0.538315     -0.488779  ...   -0.277185   \n",
      "129    0.401269       0.826108       0.345899     -0.488779  ...   -1.412466   \n",
      "\n",
      "     cg23270529  cg23530553  cg25197194  cg27285720  rs5936512  hsa.mir.1248  \\\n",
      "49     0.765124   -1.370930   -1.004264   -0.631413   1.499626      0.111155   \n",
      "106    0.808854   -0.733390    0.847146   -0.607814  -0.861248      0.076760   \n",
      "129    0.563846   -1.540623   -0.613879   -0.697913   1.479665      0.355757   \n",
      "\n",
      "     hsa.mir.154  hsa.mir.155  Death  \n",
      "49     -1.224332     0.999280    1.0  \n",
      "106    -1.224332    -0.003050    1.0  \n",
      "129    -1.224332     1.462018    0.0  \n",
      "\n",
      "[3 rows x 68 columns]\n",
      "\n",
      "\n",
      "Block 47:\n",
      "     AREG.374  BARHL1.56751  C14orf165.414767  CAPN6.827  CCL22.6367  \\\n",
      "5    0.735559      0.788205         -0.617792  -0.496939    0.631934   \n",
      "139  0.746080      0.677528         -0.617792   1.782799    0.467562   \n",
      "53   0.862464     -1.491769          1.584818   2.470384    0.821377   \n",
      "\n",
      "     CDH5.1003  DLK1.8788  FAM157B.100132403  FLJ34503.285759  \\\n",
      "5     0.950793   1.175928           0.727553         0.566900   \n",
      "139   0.685805   1.417223           0.811874         0.652588   \n",
      "53    0.415910   0.754546           0.662404         0.599936   \n",
      "\n",
      "     FLJ39609.100130417  ...  cg21822187  cg22151446  cg23042151  cg26806779  \\\n",
      "5              0.990487  ...    0.329005    0.394664    0.204416   -0.851611   \n",
      "139            1.004248  ...   -0.427794    0.325676   -1.014036   -0.572979   \n",
      "53            -0.839769  ...   -1.165825   -0.239287    0.694492   -1.096536   \n",
      "\n",
      "     cg26880735  hsa.mir.142  hsa.mir.153.2  hsa.mir.376c  hsa.mir.665  Death  \n",
      "5      0.082045    -0.356980       0.394125      1.040960    -0.355721    1.0  \n",
      "139    0.543020     0.243762       0.274003      1.036967    -0.355721    1.0  \n",
      "53     0.304330     0.341419       0.557143      0.607280    -0.355721    1.0  \n",
      "\n",
      "[3 rows x 49 columns]\n",
      "\n",
      "\n",
      "Block 48:\n",
      "     C14orf165.414767  CCL26.10344  CLCA4.22802  DUSP5P.574029  \\\n",
      "135         -0.617792    -0.410577    -0.797991       0.507767   \n",
      "143         -0.617792    -0.410577     1.516894       0.049349   \n",
      "131         -0.617792    -0.410577     1.215422      -2.515734   \n",
      "\n",
      "     FAM157B.100132403  HRH4.59340  KIAA0087.9808  MCF2L.23263  OR7A5.26659  \\\n",
      "135           0.208294    0.054273       0.302797     0.315046    -0.297328   \n",
      "143           0.601960   -0.518769       1.352170     0.686602    -0.297328   \n",
      "131           0.231864   -0.123779       0.682164     0.792293    -0.297328   \n",
      "\n",
      "     PLCB4.5332  ...  cg20927661  cg22151446  cg22157027  cg23248424  \\\n",
      "135    0.017325  ...   -0.882746   -1.040277    0.814743    0.642319   \n",
      "143    0.417545  ...   -0.272764   -1.025048    1.055481   -1.237616   \n",
      "131    0.080480  ...   -1.038546   -1.358414    1.257764    1.234048   \n",
      "\n",
      "     cg23530553  cg24123198  cg24790297  hsa.let.7a.1  hsa.mir.588  Death  \n",
      "135    0.023062   -0.565832    0.019344      1.872647    -0.244828    NaN  \n",
      "143   -0.175869    0.797429    0.513374      0.675429    -0.244828    1.0  \n",
      "131   -0.858413    1.050653    0.589390      0.318778    -0.244828    0.0  \n",
      "\n",
      "[3 rows x 47 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first few rows of each DataFrame\n",
    "for i, df in enumerate(block_dataframes):\n",
    "    print(f\"Block {i}:\")\n",
    "    print(df.head())\n",
    "    print(\"\\n\")\n",
    "    df.to_csv(f\"qubic/blocks/Block {i}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./qubic/blocks/Block 0.csv...\n",
      "Loading ./qubic/blocks/Block 1.csv...\n",
      "Loading ./qubic/blocks/Block 2.csv...\n",
      "Loading ./qubic/blocks/Block 3.csv...\n",
      "Loading ./qubic/blocks/Block 4.csv...\n",
      "Loading ./qubic/blocks/Block 5.csv...\n",
      "Loading ./qubic/blocks/Block 6.csv...\n",
      "Loading ./qubic/blocks/Block 7.csv...\n",
      "Loading ./qubic/blocks/Block 8.csv...\n",
      "Loading ./qubic/blocks/Block 9.csv...\n",
      "Loading ./qubic/blocks/Block 10.csv...\n",
      "Loading ./qubic/blocks/Block 11.csv...\n",
      "Loading ./qubic/blocks/Block 12.csv...\n",
      "Loading ./qubic/blocks/Block 13.csv...\n",
      "Loading ./qubic/blocks/Block 14.csv...\n",
      "Loading ./qubic/blocks/Block 15.csv...\n",
      "Loading ./qubic/blocks/Block 16.csv...\n",
      "Loading ./qubic/blocks/Block 17.csv...\n",
      "Loading ./qubic/blocks/Block 18.csv...\n",
      "Loading ./qubic/blocks/Block 19.csv...\n",
      "Loading ./qubic/blocks/Block 20.csv...\n",
      "Loading ./qubic/blocks/Block 21.csv...\n",
      "Loading ./qubic/blocks/Block 22.csv...\n",
      "Loading ./qubic/blocks/Block 23.csv...\n",
      "Loading ./qubic/blocks/Block 24.csv...\n",
      "Loading ./qubic/blocks/Block 25.csv...\n",
      "Loading ./qubic/blocks/Block 26.csv...\n",
      "Loading ./qubic/blocks/Block 27.csv...\n",
      "Loading ./qubic/blocks/Block 28.csv...\n",
      "Loading ./qubic/blocks/Block 29.csv...\n",
      "Loading ./qubic/blocks/Block 30.csv...\n",
      "Loading ./qubic/blocks/Block 31.csv...\n",
      "Loading ./qubic/blocks/Block 32.csv...\n",
      "Loading ./qubic/blocks/Block 33.csv...\n",
      "Loading ./qubic/blocks/Block 34.csv...\n",
      "Loading ./qubic/blocks/Block 35.csv...\n",
      "Loading ./qubic/blocks/Block 36.csv...\n",
      "Loading ./qubic/blocks/Block 37.csv...\n",
      "Loading ./qubic/blocks/Block 38.csv...\n",
      "Loading ./qubic/blocks/Block 39.csv...\n",
      "Loading ./qubic/blocks/Block 40.csv...\n",
      "Loading ./qubic/blocks/Block 41.csv...\n",
      "Loading ./qubic/blocks/Block 42.csv...\n",
      "Loading ./qubic/blocks/Block 43.csv...\n",
      "Loading ./qubic/blocks/Block 44.csv...\n",
      "Loading ./qubic/blocks/Block 45.csv...\n",
      "Loading ./qubic/blocks/Block 46.csv...\n",
      "Loading ./qubic/blocks/Block 47.csv...\n",
      "Loading ./qubic/blocks/Block 48.csv...\n",
      "Missing values in 'Death' column: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BioGptTokenizer'. \n",
      "The class this function is called from is 'GPT2Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings using biogpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type biogpt to instantiate a model of type gpt2. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of GPT2Model were not initialized from the model checkpoint at microsoft/biogpt and are newly initialized: ['h.0.attn.c_attn.bias', 'h.0.attn.c_attn.weight', 'h.0.attn.c_proj.bias', 'h.0.attn.c_proj.weight', 'h.0.ln_1.bias', 'h.0.ln_1.weight', 'h.0.ln_2.bias', 'h.0.ln_2.weight', 'h.0.mlp.c_fc.bias', 'h.0.mlp.c_fc.weight', 'h.0.mlp.c_proj.bias', 'h.0.mlp.c_proj.weight', 'h.1.attn.c_attn.bias', 'h.1.attn.c_attn.weight', 'h.1.attn.c_proj.bias', 'h.1.attn.c_proj.weight', 'h.1.ln_1.bias', 'h.1.ln_1.weight', 'h.1.ln_2.bias', 'h.1.ln_2.weight', 'h.1.mlp.c_fc.bias', 'h.1.mlp.c_fc.weight', 'h.1.mlp.c_proj.bias', 'h.1.mlp.c_proj.weight', 'h.10.attn.c_attn.bias', 'h.10.attn.c_attn.weight', 'h.10.attn.c_proj.bias', 'h.10.attn.c_proj.weight', 'h.10.ln_1.bias', 'h.10.ln_1.weight', 'h.10.ln_2.bias', 'h.10.ln_2.weight', 'h.10.mlp.c_fc.bias', 'h.10.mlp.c_fc.weight', 'h.10.mlp.c_proj.bias', 'h.10.mlp.c_proj.weight', 'h.11.attn.c_attn.bias', 'h.11.attn.c_attn.weight', 'h.11.attn.c_proj.bias', 'h.11.attn.c_proj.weight', 'h.11.ln_1.bias', 'h.11.ln_1.weight', 'h.11.ln_2.bias', 'h.11.ln_2.weight', 'h.11.mlp.c_fc.bias', 'h.11.mlp.c_fc.weight', 'h.11.mlp.c_proj.bias', 'h.11.mlp.c_proj.weight', 'h.12.attn.c_attn.bias', 'h.12.attn.c_attn.weight', 'h.12.attn.c_proj.bias', 'h.12.attn.c_proj.weight', 'h.12.ln_1.bias', 'h.12.ln_1.weight', 'h.12.ln_2.bias', 'h.12.ln_2.weight', 'h.12.mlp.c_fc.bias', 'h.12.mlp.c_fc.weight', 'h.12.mlp.c_proj.bias', 'h.12.mlp.c_proj.weight', 'h.13.attn.c_attn.bias', 'h.13.attn.c_attn.weight', 'h.13.attn.c_proj.bias', 'h.13.attn.c_proj.weight', 'h.13.ln_1.bias', 'h.13.ln_1.weight', 'h.13.ln_2.bias', 'h.13.ln_2.weight', 'h.13.mlp.c_fc.bias', 'h.13.mlp.c_fc.weight', 'h.13.mlp.c_proj.bias', 'h.13.mlp.c_proj.weight', 'h.14.attn.c_attn.bias', 'h.14.attn.c_attn.weight', 'h.14.attn.c_proj.bias', 'h.14.attn.c_proj.weight', 'h.14.ln_1.bias', 'h.14.ln_1.weight', 'h.14.ln_2.bias', 'h.14.ln_2.weight', 'h.14.mlp.c_fc.bias', 'h.14.mlp.c_fc.weight', 'h.14.mlp.c_proj.bias', 'h.14.mlp.c_proj.weight', 'h.15.attn.c_attn.bias', 'h.15.attn.c_attn.weight', 'h.15.attn.c_proj.bias', 'h.15.attn.c_proj.weight', 'h.15.ln_1.bias', 'h.15.ln_1.weight', 'h.15.ln_2.bias', 'h.15.ln_2.weight', 'h.15.mlp.c_fc.bias', 'h.15.mlp.c_fc.weight', 'h.15.mlp.c_proj.bias', 'h.15.mlp.c_proj.weight', 'h.16.attn.c_attn.bias', 'h.16.attn.c_attn.weight', 'h.16.attn.c_proj.bias', 'h.16.attn.c_proj.weight', 'h.16.ln_1.bias', 'h.16.ln_1.weight', 'h.16.ln_2.bias', 'h.16.ln_2.weight', 'h.16.mlp.c_fc.bias', 'h.16.mlp.c_fc.weight', 'h.16.mlp.c_proj.bias', 'h.16.mlp.c_proj.weight', 'h.17.attn.c_attn.bias', 'h.17.attn.c_attn.weight', 'h.17.attn.c_proj.bias', 'h.17.attn.c_proj.weight', 'h.17.ln_1.bias', 'h.17.ln_1.weight', 'h.17.ln_2.bias', 'h.17.ln_2.weight', 'h.17.mlp.c_fc.bias', 'h.17.mlp.c_fc.weight', 'h.17.mlp.c_proj.bias', 'h.17.mlp.c_proj.weight', 'h.18.attn.c_attn.bias', 'h.18.attn.c_attn.weight', 'h.18.attn.c_proj.bias', 'h.18.attn.c_proj.weight', 'h.18.ln_1.bias', 'h.18.ln_1.weight', 'h.18.ln_2.bias', 'h.18.ln_2.weight', 'h.18.mlp.c_fc.bias', 'h.18.mlp.c_fc.weight', 'h.18.mlp.c_proj.bias', 'h.18.mlp.c_proj.weight', 'h.19.attn.c_attn.bias', 'h.19.attn.c_attn.weight', 'h.19.attn.c_proj.bias', 'h.19.attn.c_proj.weight', 'h.19.ln_1.bias', 'h.19.ln_1.weight', 'h.19.ln_2.bias', 'h.19.ln_2.weight', 'h.19.mlp.c_fc.bias', 'h.19.mlp.c_fc.weight', 'h.19.mlp.c_proj.bias', 'h.19.mlp.c_proj.weight', 'h.2.attn.c_attn.bias', 'h.2.attn.c_attn.weight', 'h.2.attn.c_proj.bias', 'h.2.attn.c_proj.weight', 'h.2.ln_1.bias', 'h.2.ln_1.weight', 'h.2.ln_2.bias', 'h.2.ln_2.weight', 'h.2.mlp.c_fc.bias', 'h.2.mlp.c_fc.weight', 'h.2.mlp.c_proj.bias', 'h.2.mlp.c_proj.weight', 'h.20.attn.c_attn.bias', 'h.20.attn.c_attn.weight', 'h.20.attn.c_proj.bias', 'h.20.attn.c_proj.weight', 'h.20.ln_1.bias', 'h.20.ln_1.weight', 'h.20.ln_2.bias', 'h.20.ln_2.weight', 'h.20.mlp.c_fc.bias', 'h.20.mlp.c_fc.weight', 'h.20.mlp.c_proj.bias', 'h.20.mlp.c_proj.weight', 'h.21.attn.c_attn.bias', 'h.21.attn.c_attn.weight', 'h.21.attn.c_proj.bias', 'h.21.attn.c_proj.weight', 'h.21.ln_1.bias', 'h.21.ln_1.weight', 'h.21.ln_2.bias', 'h.21.ln_2.weight', 'h.21.mlp.c_fc.bias', 'h.21.mlp.c_fc.weight', 'h.21.mlp.c_proj.bias', 'h.21.mlp.c_proj.weight', 'h.22.attn.c_attn.bias', 'h.22.attn.c_attn.weight', 'h.22.attn.c_proj.bias', 'h.22.attn.c_proj.weight', 'h.22.ln_1.bias', 'h.22.ln_1.weight', 'h.22.ln_2.bias', 'h.22.ln_2.weight', 'h.22.mlp.c_fc.bias', 'h.22.mlp.c_fc.weight', 'h.22.mlp.c_proj.bias', 'h.22.mlp.c_proj.weight', 'h.23.attn.c_attn.bias', 'h.23.attn.c_attn.weight', 'h.23.attn.c_proj.bias', 'h.23.attn.c_proj.weight', 'h.23.ln_1.bias', 'h.23.ln_1.weight', 'h.23.ln_2.bias', 'h.23.ln_2.weight', 'h.23.mlp.c_fc.bias', 'h.23.mlp.c_fc.weight', 'h.23.mlp.c_proj.bias', 'h.23.mlp.c_proj.weight', 'h.3.attn.c_attn.bias', 'h.3.attn.c_attn.weight', 'h.3.attn.c_proj.bias', 'h.3.attn.c_proj.weight', 'h.3.ln_1.bias', 'h.3.ln_1.weight', 'h.3.ln_2.bias', 'h.3.ln_2.weight', 'h.3.mlp.c_fc.bias', 'h.3.mlp.c_fc.weight', 'h.3.mlp.c_proj.bias', 'h.3.mlp.c_proj.weight', 'h.4.attn.c_attn.bias', 'h.4.attn.c_attn.weight', 'h.4.attn.c_proj.bias', 'h.4.attn.c_proj.weight', 'h.4.ln_1.bias', 'h.4.ln_1.weight', 'h.4.ln_2.bias', 'h.4.ln_2.weight', 'h.4.mlp.c_fc.bias', 'h.4.mlp.c_fc.weight', 'h.4.mlp.c_proj.bias', 'h.4.mlp.c_proj.weight', 'h.5.attn.c_attn.bias', 'h.5.attn.c_attn.weight', 'h.5.attn.c_proj.bias', 'h.5.attn.c_proj.weight', 'h.5.ln_1.bias', 'h.5.ln_1.weight', 'h.5.ln_2.bias', 'h.5.ln_2.weight', 'h.5.mlp.c_fc.bias', 'h.5.mlp.c_fc.weight', 'h.5.mlp.c_proj.bias', 'h.5.mlp.c_proj.weight', 'h.6.attn.c_attn.bias', 'h.6.attn.c_attn.weight', 'h.6.attn.c_proj.bias', 'h.6.attn.c_proj.weight', 'h.6.ln_1.bias', 'h.6.ln_1.weight', 'h.6.ln_2.bias', 'h.6.ln_2.weight', 'h.6.mlp.c_fc.bias', 'h.6.mlp.c_fc.weight', 'h.6.mlp.c_proj.bias', 'h.6.mlp.c_proj.weight', 'h.7.attn.c_attn.bias', 'h.7.attn.c_attn.weight', 'h.7.attn.c_proj.bias', 'h.7.attn.c_proj.weight', 'h.7.ln_1.bias', 'h.7.ln_1.weight', 'h.7.ln_2.bias', 'h.7.ln_2.weight', 'h.7.mlp.c_fc.bias', 'h.7.mlp.c_fc.weight', 'h.7.mlp.c_proj.bias', 'h.7.mlp.c_proj.weight', 'h.8.attn.c_attn.bias', 'h.8.attn.c_attn.weight', 'h.8.attn.c_proj.bias', 'h.8.attn.c_proj.weight', 'h.8.ln_1.bias', 'h.8.ln_1.weight', 'h.8.ln_2.bias', 'h.8.ln_2.weight', 'h.8.mlp.c_fc.bias', 'h.8.mlp.c_fc.weight', 'h.8.mlp.c_proj.bias', 'h.8.mlp.c_proj.weight', 'h.9.attn.c_attn.bias', 'h.9.attn.c_attn.weight', 'h.9.attn.c_proj.bias', 'h.9.attn.c_proj.weight', 'h.9.ln_1.bias', 'h.9.ln_1.weight', 'h.9.ln_2.bias', 'h.9.ln_2.weight', 'h.9.mlp.c_fc.bias', 'h.9.mlp.c_fc.weight', 'h.9.mlp.c_proj.bias', 'h.9.mlp.c_proj.weight', 'ln_f.bias', 'ln_f.weight', 'wpe.weight', 'wte.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Erreur lors de la tokenisation à l'index 0: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 1: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 2: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 3: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 4: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 5: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 6: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 7: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 8: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 9: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 10: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 11: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 12: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 13: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 14: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 15: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 16: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 17: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 18: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 19: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 20: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 21: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 22: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 23: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 24: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 25: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 26: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 27: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 28: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 29: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 30: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 31: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 32: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 33: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 34: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 35: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 36: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 37: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 38: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 39: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 40: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 41: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 42: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 43: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 44: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 45: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 46: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 47: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 48: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 49: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 50: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 51: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 52: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 53: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 54: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 55: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 56: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 57: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 58: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 59: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 60: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 61: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 62: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 63: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 64: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 65: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 66: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 67: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 68: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 69: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 70: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 71: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 72: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 73: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 74: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 75: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 76: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 77: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 78: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 79: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 80: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 81: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 82: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 83: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 84: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 85: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 86: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 87: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 88: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 89: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 90: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 91: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 92: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 93: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 94: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 95: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 96: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 97: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 98: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 99: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 100: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 101: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 102: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 103: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 104: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 105: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 106: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 107: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 108: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 109: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 110: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 111: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 112: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 113: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 114: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 115: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 116: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 117: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 118: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 119: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 120: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 121: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 122: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 123: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 124: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 125: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 126: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 127: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 128: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 129: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 130: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 131: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 132: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 133: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 134: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 135: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 136: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 137: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 138: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 139: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 140: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 141: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 142: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 143: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 144: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 145: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 146: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 147: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 148: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 149: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 150: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 151: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 152: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 153: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 154: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 155: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 156: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 157: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 158: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 159: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 160: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 161: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 162: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 163: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 164: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 165: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 166: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 167: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 168: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 169: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 170: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 171: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 172: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 173: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 174: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 175: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 176: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 177: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 178: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 179: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 180: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 181: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 182: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 183: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 184: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 185: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 186: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 187: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 188: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 189: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 190: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 191: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 192: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 193: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 194: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 195: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 196: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 197: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 198: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 199: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 200: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 201: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 202: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 203: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 204: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 205: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 206: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 207: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 208: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 209: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 210: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 211: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 212: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 213: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 214: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 215: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 216: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 217: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 218: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 219: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 220: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 221: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 222: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 223: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 224: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 225: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 226: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 227: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 228: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 229: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 230: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 231: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 232: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 233: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 234: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 235: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 236: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 237: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 238: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 239: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 240: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 241: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 242: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 243: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 244: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 245: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 246: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 247: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 248: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 249: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 250: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 251: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 252: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 253: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 254: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 255: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 256: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 257: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 258: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 259: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 260: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 261: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 262: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 263: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 264: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 265: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 266: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 267: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 268: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 269: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 270: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 271: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 272: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 273: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 274: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 275: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 276: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 277: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 278: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 279: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 280: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 281: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 282: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 283: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 284: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 285: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 286: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 287: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 288: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 289: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 290: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 291: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 292: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 293: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 294: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 295: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 296: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 297: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 298: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 299: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 300: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 301: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 302: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 303: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 304: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 305: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 306: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 307: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 308: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 309: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 310: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 311: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 312: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 313: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 314: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 315: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 316: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 317: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 318: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 319: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 320: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 321: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 322: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 323: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 324: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 325: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 326: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 327: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 328: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 329: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 330: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 331: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 332: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 333: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 334: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 335: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 336: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 337: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 338: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 339: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 340: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 341: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 342: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 343: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 344: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 345: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 346: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 347: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 348: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 349: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 350: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 351: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 352: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 353: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 354: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 355: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 356: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 357: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 358: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 359: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 360: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 361: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 362: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 363: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 364: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 365: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 366: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 367: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 368: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 369: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 370: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 371: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 372: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 373: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 374: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 375: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 376: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 377: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 378: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 379: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 380: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 381: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 382: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 383: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 384: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 385: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 386: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 387: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 388: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 389: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 390: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 391: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 392: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 393: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 394: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 395: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 396: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 397: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 398: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 399: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 400: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 401: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 402: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 403: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 404: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 405: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 406: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 407: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 408: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 409: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 410: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 411: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 412: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 413: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 414: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 415: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 416: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 417: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 418: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 419: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 420: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 421: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 422: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 423: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 424: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 425: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 426: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 427: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 428: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 429: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 430: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 431: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 432: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 433: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 434: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 435: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 436: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 437: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 438: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 439: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 440: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 441: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 442: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 443: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 444: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 445: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 446: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 447: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 448: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 449: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 450: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 451: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 452: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 453: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 454: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 455: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 456: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 457: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 458: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 459: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 460: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 461: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 462: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 463: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 464: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 465: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 466: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 467: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 468: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 469: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 470: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 471: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 472: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 473: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 474: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 475: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 476: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 477: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 478: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 479: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 480: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 481: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 482: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 483: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 484: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 485: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 486: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 487: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 488: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 489: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 490: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 491: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 492: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 493: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 494: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 495: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 496: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 497: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 498: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 499: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 500: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 501: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 502: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 503: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 504: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 505: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 506: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 507: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 508: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 509: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 510: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 511: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 512: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 513: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 514: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 515: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 516: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 517: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 518: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 519: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 520: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 521: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 522: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 523: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 524: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 525: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 526: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 527: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 528: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 529: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 530: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 531: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 532: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 533: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 534: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 535: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 536: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 537: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 538: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 539: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 540: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 541: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 542: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 543: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 544: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 545: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 546: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 547: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 548: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 549: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 550: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 551: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 552: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 553: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 554: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 555: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 556: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 557: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 558: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 559: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 560: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 561: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 562: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 563: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 564: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 565: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 566: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 567: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 568: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 569: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 570: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 571: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 572: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 573: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 574: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 575: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 576: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 577: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 578: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 579: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 580: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 581: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 582: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 583: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 584: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 585: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 586: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 587: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 588: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 589: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 590: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 591: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 592: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 593: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 594: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 595: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 596: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 597: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 598: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 599: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 600: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 601: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 602: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 603: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 604: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 605: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 606: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 607: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 608: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 609: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 610: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 611: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 612: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 613: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 614: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 615: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 616: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 617: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 618: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 619: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 620: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 621: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 622: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 623: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 624: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 625: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 626: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 627: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 628: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 629: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 630: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 631: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 632: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 633: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 634: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 635: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 636: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 637: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 638: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 639: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 640: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 641: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 642: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 643: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 644: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 645: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 646: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 647: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 648: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "❌ Erreur lors de la tokenisation à l'index 649: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "Embedding generation time: 8.90 seconds\n",
      "\n",
      "Results for biogpt:\n",
      "Training Time: 0.05 seconds\n",
      "Accuracy: 0.6461538461538462\n",
      "ROC-AUC Score: 0.5\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        23\n",
      "         1.0       0.65      1.00      0.79        42\n",
      "\n",
      "    accuracy                           0.65        65\n",
      "   macro avg       0.32      0.50      0.39        65\n",
      "weighted avg       0.42      0.65      0.51        65\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings using biobert...\n",
      "Embedding generation time: 202.54 seconds\n",
      "\n",
      "Results for biobert:\n",
      "Training Time: 1.78 seconds\n",
      "Accuracy: 0.8\n",
      "ROC-AUC Score: 0.8540372670807453\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.57      0.67        23\n",
      "         1.0       0.80      0.93      0.86        42\n",
      "\n",
      "    accuracy                           0.80        65\n",
      "   macro avg       0.80      0.75      0.76        65\n",
      "weighted avg       0.80      0.80      0.79        65\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings using scbert...\n",
      "Embedding generation time: 207.68 seconds\n",
      "\n",
      "Results for scbert:\n",
      "Training Time: 1.98 seconds\n",
      "Accuracy: 0.7846153846153846\n",
      "ROC-AUC Score: 0.84472049689441\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.61      0.67        23\n",
      "         1.0       0.80      0.88      0.84        42\n",
      "\n",
      "    accuracy                           0.78        65\n",
      "   macro avg       0.77      0.74      0.75        65\n",
      "weighted avg       0.78      0.78      0.78        65\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Models:\n",
      "     Model  Embedding Time (s)  Training Time (s)  Accuracy   ROC-AUC\n",
      "0   biogpt            8.901112           0.046050  0.646154  0.500000\n",
      "1  biobert          202.544367           1.778653  0.800000  0.854037\n",
      "2   scbert          207.684438           1.981996  0.784615  0.844720\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2Model\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Step 1: Load and Combine All Blocks\n",
    "def load_and_combine_blocks(block_range, base_path=\"\"):\n",
    "    blocks = []\n",
    "    for i in block_range:\n",
    "        file_path = os.path.join(base_path, f\"Block {i}.csv\")\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"Loading {file_path}...\")\n",
    "            block = pd.read_csv(file_path)\n",
    "            blocks.append(block)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    if not blocks:\n",
    "        raise ValueError(\"No files found to concatenate. Check the directory and file names.\")\n",
    "    \n",
    "    combined_data = pd.concat(blocks, ignore_index=True)\n",
    "    return combined_data\n",
    "\n",
    "# Load blocks 0 to 48\n",
    "block_range = range(49)  \n",
    "try:\n",
    "    combined_data = load_and_combine_blocks(block_range, \"./qubic/blocks/\")\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "# Step 2: Preprocess the Data\n",
    "print(\"Missing values in 'Death' column:\", combined_data[\"Death\"].isnull().sum())\n",
    "\n",
    "# Remove rows with missing target values\n",
    "combined_data = combined_data.dropna(subset=[\"Death\"])\n",
    "\n",
    "# Separate features and target\n",
    "X = combined_data.drop(columns=[\"Death\"])\n",
    "y = combined_data[\"Death\"]\n",
    "\n",
    "# Step 3: Convert Gene Data to Text\n",
    "def convert_to_text(row, genes):\n",
    "    return \", \".join([f\"{gene}: {row[gene]}\" for gene in genes])\n",
    "\n",
    "genes = X.columns\n",
    "patient_texts = X.apply(lambda row: convert_to_text(row, genes), axis=1)\n",
    "\n",
    "# Gérer les cas où les textes sont manquants\n",
    "patient_texts = patient_texts.fillna(\"\")\n",
    "patient_texts = patient_texts.astype(str)  # Conversion en string\n",
    "\n",
    "# Step 4: Generate Embeddings Using Different Models\n",
    "def generate_embeddings(texts, model_name=\"biobert\"):\n",
    "    if model_name == \"biobert\":\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "        model = BertModel.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "    elif model_name == \"scbert\":\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "        model = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "    elif model_name == \"biogpt\":\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "        model = GPT2Model.from_pretrained(\"microsoft/biogpt\")\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model name. Choose from 'biobert', 'scbert', or 'biogpt'.\")\n",
    "\n",
    "    embeddings = []\n",
    "    for idx, text in enumerate(texts):\n",
    "        if not isinstance(text, str) or text.strip() == \"\":\n",
    "            print(f\"⚠️ Skipping invalid text at index {idx}\")\n",
    "            embeddings.append(np.zeros(768))  # Placeholder vector\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            inputs = tokenizer(\n",
    "                str(text),  # Forcer la conversion en string\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,  # Assure un padding uniforme\n",
    "                max_length=512,\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur lors de la tokenisation à l'index {idx}: {e}\")\n",
    "            embeddings.append(np.zeros(768))  # Placeholder pour éviter de casser le pipeline\n",
    "\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Step 5: Train and Evaluate Models\n",
    "def train_and_evaluate(model_name, embeddings, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        embeddings, y, test_size=0.1, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    xgb_model = XGBClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"ROC-AUC Score:\", roc_auc)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=[\"Alive\", \"Dead\"], yticklabels=[\"Alive\", \"Dead\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix for {model_name}\")\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy, roc_auc, training_time\n",
    "\n",
    "# Step 6: Compare Models\n",
    "models = [\"biogpt\", \"biobert\", \"scbert\"]\n",
    "results = []\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"\\nGenerating embeddings using {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    embeddings = generate_embeddings(patient_texts, model_name=model_name)\n",
    "    embedding_time = time.time() - start_time\n",
    "    print(f\"Embedding generation time: {embedding_time:.2f} seconds\")\n",
    "\n",
    "    accuracy, roc_auc, training_time = train_and_evaluate(model_name, embeddings, y)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Embedding Time (s)\": embedding_time,\n",
    "        \"Training Time (s)\": training_time,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"ROC-AUC\": roc_auc,\n",
    "    })\n",
    "\n",
    "# Step 7: Compare Results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nComparison of Models:\")\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
